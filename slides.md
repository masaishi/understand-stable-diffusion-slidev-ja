---
# try also 'default' to start simple
theme: seriph
colorSchema: 'light'

# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: backgrounds/understand-sd.png
# some information about your slides, markdown enabled
title: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç†è§£ã™ã‚‹Stable Diffusion
info: |
  ## Stable Diffusionã®ç”»åƒç”Ÿæˆã‚³ãƒ¼ãƒ‰ã®è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰
  diffusersã‚’ã‚·ãƒ³ãƒ—ãƒ«åŒ–ã—ãŸã€parediffusersã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’é€šã˜ã¦ã€Latent Diffusion Modelsã®ç”»åƒç”Ÿæˆã®ä»•çµ„ã¿ã‚’è§£èª¬ã™ã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã™ã€‚

author: masaishi
keywords: [ Stable Diffusion, Diffusers, parediffusers, AI, ML, Generative Models ]

export:
  format: pdf
  timeout: 30000
  dark: false
  withClicks: false

lineNumbers: true

# apply any unocss classes to the current slide
class: text-center
# https://sli.dev/custom/highlighters.html
highlighter: shiki
# https://sli.dev/guide/drawing
drawings:
  persist: true
# slide transition: https://sli.dev/guide/animations#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/guide/syntax#mdc-syntax
mdc: true

fonts:
  sans: Noto Serif JP, serif
---

# ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç†è§£ã™ã‚‹Stable Diffusion

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Understand Stable Diffusion from code, cyberpunk theme, best quality, high resolution, concept art</p>

---
src: /slides/intro.md
---

---
level: 2
layout: center
---

# Kagglerã«ã¨ã£ã¦ç”»åƒç”Ÿæˆã‚’ä½¿ã†æ©Ÿä¼šã¯å°‘ãªã„?

<img src="/images/stable-diffusion-image-to-prompts.png" class="h-100" />

<a src="https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/overview" target="_blank" class="abs-bl w-full mb-6 text-center text-xs text-black border-none!">https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/overview</a>

---
level: 2
layout: center
---

ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç›®çš„

# ç”»åƒç”Ÿæˆã®ã‚³ãƒ¼ãƒ‰ã‚’1é€šã‚Šç´¹ä»‹ã—ãŸã„ã€‚

---
level: 2
layout: center
---

# ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¤ã„ã¦
ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’æ›¸ãã«ã‚ãŸã£ã¦ã€è‡ªåˆ†ãŒç†è§£ã§ãã¦ã„ãªã‹ã£ãŸã¨ã“ã‚ãªã©ã‚’å¤šãå®Ÿæ„Ÿã—ã¾ã—ãŸã€‚é–“é•ã£ãŸèª¬æ˜ã‚’ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã€ä¸æ˜ç­ãªã¨ã“ã‚ã‚„ã€é–“é•ã„ã‚’ä¸‹è¨˜ã®ãƒªãƒ³ã‚¯ã‹ã‚‰æ•™ãˆã¦ã„ãŸã ã‘ã‚‹ã¨å¹¸ã„ã§ã™ã€‚

<br />

### [<mdi-github-circle />understand-stable-diffusion-slidev-ja](https://github.com/masaishi/understand-stable-diffusion-slidev-ja)

<br />

[<mdi-radiobox-marked />Issues](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/issues): é–“é•ã„ã‚’è¦‹ã¤ã‘ãŸã‚‰ã”æŒ‡æ‘˜ãã ã•ã„ã€‚
<br />

[<mdi-message-text-outline />Discussions](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/discussions): è³ªå•ãŒã‚ã‚Œã°ã“ã¡ã‚‰ã‹ã‚‰ãŠé¡˜ã„ã—ã¾ã™ã€‚
<br />

[<mdi-source-pull />Pull Requests](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/pulls): ã‚‚ã—ä¿®æ­£ãŒã‚ã‚Œã°ãŠé€ã‚Šãã ã•ã„ã€‚
<br />

---
layout: center
title: ç›®æ¬¡
---

# ç›®æ¬¡
<Toc minDepth="1" maxDepth="1"></Toc>

---
layout: cover
title: Stable Diffusionã®æ¦‚è¦
background: /backgrounds/stable-diffusion.png
---

# 4. Stable Diffusionã®æ¦‚è¦

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Stable Diffusion, watercolor painting, best quality, high resolution</p>

---
level: 2
layout: center
---

# Stable Diffusionã¨ã¯?

- StabilityAIã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸLatent Diffusion Model (LDM)ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«
- Text-to-Image, Image-to-Imageãªã©ã®ã‚¿ã‚¹ã‚¯ã«åˆ©ç”¨å¯èƒ½
- Diffusersã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ç°¡å˜ã«å‹•ã‹ã™ã“ã¨ãŒã§ãã‚‹ã€‚
- https://arxiv.org/abs/2112.10752

---
level: 2
layout: center
---

ã–ã£ãã‚Šã—ãŸLDMã®èª¬æ˜

<h1>1. Promptã‚’Embeddingã«å¤‰æ›ã™ã‚‹<br />
2. ãƒ©ãƒ³ãƒ€ãƒ ãªLatentã‚’ä½œã‚‹<br />
3. UNetã§ã€ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã‚’è¡Œã†<br />
4. VAEã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹</h1>

---
level: 2
layout: center
---

å®Ÿéš›ã«Text2Imgã‚’è¡Œã†ã‚³ãƒ¼ãƒ‰

<iframe frameborder="0" scrolling="no" style="width:45rem; height:163px;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2F035772c684ae8d16c7c908f185f6413b72658126%2Fsrc%2Fparediffusers%2Fpipeline.py%23L131-L134&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
layout: center
---

Latent Diffusion Model (LDM)ã¨ã¯?

# Latent Space (æ»åœ¨ç©ºé–“)ã§ã€
# <span v-mark.yellow="1">DDPM</span>ã‚’å‹•ã‹ã™ãƒ¢ãƒ‡ãƒ«

---
level: 2
---

Denoising Diffusion Probabilistic Model (DDPM)ã¨ã¯?

<h1 class="!text-7.9">ç”»åƒã«<span v-mark.red="1">ãƒã‚¤ã‚ºã‚’åŠ ãˆ</span>ã€ãã“ã‹ã‚‰<span v-mark.blue="2">å…ƒã®ç”»åƒã«å¾©å…ƒ</span>ã™ã‚‹ãƒ¢ãƒ‡ãƒ«</h1>

<p>éŸ³å£°ãªã©ã®ãƒ‡ãƒ¼ã‚¿å…¨èˆ¬ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã¯ç”»åƒã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚</p>

<ul>
	<li><span v-mark.red="1">Diffusion process(æ‹¡æ•£éç¨‹)</span>ã‚’ç”¨ã„ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’è¡Œã†ã€‚ç¢ºç‡éç¨‹ï¼ˆãƒãƒ«ã‚³ãƒ•é€£é–)</li>
	<li><span v-mark.blue="2">Reverse process(é€†æ‹¡æ•£éç¨‹)</span>ã‚’ç”¨ã„ã€ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒã™ã‚‹ã€‚</li>
</ul>

<br />

<img src="/images/ddpm-figure.png" class="abs-b mb-10 ml-auto mr-auto w-5/6" />

<!-- Reference -->
<p class="text-black text-xs abs-bl w-full mb-6 text-center">
Jonathan Ho, Ajay Jain, Pieter Abbeel: â€œDenoising Diffusion Probabilistic Modelsâ€, 2020; <a href='http://arxiv.org/abs/2006.11239'>arXiv:2006.11239</a>.
</p>

---
level: 2
layout: center
---

# Diffusionã¯å‰å‡¦ç†ã§ã‚‚NNã§ã‚‚ãªã„ã®ã«ã€
# Diffusion Modelã¨å‘¼ã°ã‚Œã‚‹ãŒé¢ç™½ã„

---
level: 2
layout: center
---

Latent Diffusion Model (LDM)ã¨ã¯?

# <span v-mark.green="1">Latent Space (æ»åœ¨ç©ºé–“)</span>ã§ã€
# DDPMã‚’å‹•ã‹ã™ãƒ¢ãƒ‡ãƒ«

---
level: 2
layout: center
transition: fade
---

ç›®çš„é–¢æ•°é–“é•ã„æ¢ã—

<h2>
$$
L_{DM} := \mathbb{E}_{x, \epsilon \sim \mathcal{N}(0, 1),  t}\Big[ \Vert \epsilon - \epsilon_\theta(x_{t},t) \Vert_{2}^{2}\Big] \, .
$$
</h2>

<h2>
$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0, 1),  t}\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t) \Vert_{2}^{2}\Big] \, .
$$
</h2>

<!--
ä¸ŠãŒDiffusion Model
ä¸‹ãŒLatent Diffusion Model

\mathcal  = ç­†è¨˜ä½“(ã‚«ãƒªã‚°ãƒ©ãƒ•ã‚£ãƒ¼)ã€ãªã‚“ã¦èª­ã‚ã°ã„ã„? AIã«ãŠã„ã¦ã¯Encorderã¨èª­ã‚“ã˜ã‚ƒã£ã¦ã„ã„?
\mathbb{E}_{x ã§ã¯ãªãã€VAE Encorderã‚’é€šã—ãŸlatent spaceã‚’æœŸå¾…å€¤ã®è¨ˆç®—ã«ä½¿ã£ã¦ã„ã‚‹ã€‚
-->

---
level: 2
layout: center
transition: fade
---

Latent Diffusion Model (LDM)

<h2>
$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0, 1),  t}\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t) \Vert_{2}^{2}\Big] \, .
$$
</h2>

<!--
\mathcal{E} = VAE Encorder
\mathcal{N} = ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚º
\epsilon_\theta = VAE Decoder

E(æœŸå¾…å€¤): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ ãŒEncorderã‚’é€šã—ã¦latent spaceã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ã€‚
VAE Decoderã‚’é€šã™ã¨ãã€t=timestepã‚’è€ƒæ…®ã—ã¦ã„ã‚‹ã€‚
-->

---
level: 2
layout: center
transition: fade
---

Latent Diffusion Model (LDM)

<h2>
$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0, 1),  t}\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t) \Vert_{2}^{2}\Big] \, .
$$
</h2>

<!--
\mathcal{E} = VAE Encorder
\mathcal{N} = ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚º
\epsilon_\theta = VAE Decoder

E(æœŸå¾…å€¤): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ ãŒEncorderã‚’é€šã—ã¦latent spaceã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ã€‚
VAE Decoderã‚’é€šã™ã¨ãã€t=timestepã‚’è€ƒæ…®ã—ã¦ã„ã‚‹ã€‚
-->

---
level: 2
layout: center
---

Latent Diffusion Model (LDM) with Conditioning

<h2>
$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0, 1), t }\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t, \tau_\theta(y)) \Vert_{2}^{2}\Big] \, ,
$$
</h2>

<v-clicks every="1" at="1">

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) \cdot V
$$

$$
\begin{equation*}
Q = W^{(i)}_Q \cdot  \varphi_i(z_t), \; K = W^{(i)}_K \cdot \tau_\theta(y),
  \; V = W^{(i)}_V \cdot \tau_\theta(y) . \nonumber
%
\end{equation*}
$$

</v-clicks>

<!--
Conditioningã€ã¤ã¾ã‚Špromptã‚„semantic mapã€repres entations, imagesãªã©ã‚’è€ƒæ…®ã—ã¦ã„ã‚‹ã€‚
-->

---
level: 2
---

<div class="flex flex-col !justify-between w-full h-120">
	<div>
		<img src="/images/ddpm-figure.png" class="ml-auto mr-auto h-26" />
		<!-- Reference -->
		<p class="text-black text-xs w-full mt-6 text-center">
		Jonathan Ho, Ajay Jain, Pieter Abbeel: â€œDenoising Diffusion Probabilistic Modelsâ€, 2020; <a href='http://arxiv.org/abs/2006.11239'>arXiv:2006.11239</a>.
		</p>
	</div>
	<div v-click>
		<span class="text-xs ml-27.5% mt-0 mb-0">Transformerã®æ¬¡ã«æ­»ã¬ã»ã©ç›®ã«ã—ãŸStable Diffusionã®å›³</span>
		<img src="/images/stable-diffusion-figure.png" alt="Stable Diffusion Figure" class="ml-auto mr-auto h-48 object-contain" />
		<p class="text-black text-xs w-full mt-6 text-center">
		Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer: â€œHigh-Resolution Image Synthesis with Latent Diffusion Modelsâ€, 2021; <a href='http://arxiv.org/abs/2112.10752'>arXiv:2112.10752</a>.
		</p>
	</div>
</div>

---
level: 2
layout: center
---

Latent Space(æ»åœ¨ç©ºé–“)ã¨ã¯?

# å…¥åŠ›ç”»åƒã®ç‰¹å¾´ã‚’æŠ½å‡ºã—ãŸç©ºé–“

<!--
TODO: VAEã‚’é€šã—ãŸç”»åƒã®å¹³å‡ã‚’ã¨ã£ãŸç”»åƒã‚’ç”¨æ„ã™ã‚‹ã€‚
-->

---
level: 2
layout: center
---

ã–ã£ãã‚Šã—ãŸèª¬æ˜

<h1>1. Promptã‚’Embeddingã«å¤‰æ›ã™ã‚‹<br />
2. ãƒ©ãƒ³ãƒ€ãƒ ãªLatentã‚’ä½œã‚‹<br />
3. UNetã§ã€ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã‚’è¡Œã†<br />
4. VAEã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹</h1>

---
level: 2
layout: center
---

# Diffusersã¨ã¯?

- Hugging FaceğŸ¤—ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸDiffusion Modelsã‚’æ‰±ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«å‹•ã‹ã™ã“ã¨ãŒã§ãã‚‹ã€‚
- <mdi-github-circle /> https://github.com/huggingface/diffusers

---
level: 2
layout: image-right
image: /exps/d-sd2-sample-42.png
---

# [<mdi-github-circle />Diffusers](https://github.com/huggingface/diffusers)ã‚’è©¦ã™
## <!-- TODO: Find better way, currently for avoide below becomes subtitle -->

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1EbqeoWL5kPaDA8INLWl8g34v3vn83AQ5?usp=sharing)

Install the Diffusers library:
```python
!pip install transformers diffusers accelerate -U
```

Generate an image from text:
```python {all|4-7|8|10|all}{lines:true}
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
  "stabilityai/stable-diffusion-2",
  dtype=torch.float16,
).to(device=torch.device("cuda"))
prompt = "painting depicting the sea, sunrise, ship, artstation, 4k, concept art"

image = pipe(prompt, width=512, height=512).images[0]
display(image)
```

---
level: 2
layout: center
---

# Diffusersã¯æ©Ÿèƒ½ãŒè±Šå¯Œã§æŸ”è»Ÿæ€§ã‚‚é«˜ã„ãŒã€<br />
# ãã®åˆ†ã‚³ãƒ¼ãƒ‰ã®ç†è§£ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã€‚

---
level: 2
---

# [<mdi-github-circle />diffusers/.../pipeline_stable_diffusion.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll mt-10" style="width:100%; height:85%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fdiffusers%2Fblob%2Fmain%2Fsrc%2Fdiffusers%2Fpipelines%2Fstable_diffusion%2Fpipeline_stable_diffusion.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

# [<mdi-github-circle />parediffusers/.../pipeline.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/pipeline.py)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll mt-10" style="width:100%; height:85%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fpipeline.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
layout: image-right
image: /exps/p-sd2-sample-43.png
---

# [<mdi-github-circle />PareDiffusers](https://github.com/masaishi/parediffusers)
## <!-- TODO: Find better way, currently for avoide below becomes subtitle -->

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1I-qU3hfF19T42ksIh5FC0ReyKZ2hsJvx?usp=sharing)

Install the PareDiffusers library:
```python
!pip install parediffusers
```

Generate an image from text:
```python {all|2|4|6}{lines:true}
import torch
from parediffusers import PareDiffusionPipeline

pipe = PareDiffusionPipeline.from_pretrained(
  "stabilityai/stable-diffusion-2",
  device=torch.device("cuda"),
  dtype=torch.float16,
)
prompt = "painting depicting the sea, sunrise, ship, artstation, 4k, concept art"

image = pipe(prompt, width=512, height=512)
display(image)
```

---
layout: cover
title: Pipeline
background: /backgrounds/pipeline.png
---

# 5. Pipeline

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Pipeline, cyberpunk theme, best quality, high resolution, concept art</p>

---
level: 2
layout: image
image: /images/stable-diffusion-figure.png
backgroundSize: 70%
class: 'text-black'
---

<!-- Reference -->
<p class="text-black text-xs abs-bl w-full mb-6 text-center">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer: â€œHigh-Resolution Image Synthesis with Latent Diffusion Modelsâ€, 2021; <a href='http://arxiv.org/abs/2112.10752'>arXiv:2112.10752</a>.
</p>

---
level: 2
layout: center
---

<iframe frameborder="0" scrolling="no" style="width:100%; height:163px;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2F035772c684ae8d16c7c908f185f6413b72658126%2Fsrc%2Fparediffusers%2Fpipeline.py%23L131-L134&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<div class="w-full flex flex-col justify-center mt-10">
<img src="/images/stable-diffusion-figure.png" alt="Stable Diffusion Figure" class="h-48 object-contain" />
<p class="text-black text-xs w-full mt-6 text-center">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer: â€œHigh-Resolution Image Synthesis with Latent Diffusion Modelsâ€, 2021; <a href='http://arxiv.org/abs/2112.10752'>arXiv:2112.10752</a>.
</p>
</div>

---
level: 2
layout: center
---

````md magic-move
```python {all}{lines:true}
prompt_embeds = self.encode_prompt(prompt)
latents = self.get_latent(width, height).unsqueeze(dim=0)
latents = self.denoise(latents, prompt_embeds, num_inference_steps, guidance_scale)
image = self.vae_decode(latents)
```
```md {all|1|2|3|4|all}
1. `encode_prompt` : ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’embeddingã«å¤‰æ›ã™ã‚‹ã€‚
2. `get_latent` : ç”Ÿæˆã—ãŸã„ç”»åƒã‚µã‚¤ã‚ºã®ã€1/8ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
3. `denoise` : ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®embeddingã‹ã‚‰ã€æ½œåœ¨ç©ºé–“ã‚’åå¾©çš„ã«ãƒ‡ãƒã‚¤ã‚ºã™ã‚‹ã€‚
4. `vae_decode` : ãƒ‡ãƒã‚¤ã‚ºã•ã‚ŒãŸæ½œåœ¨ç©ºé–“ã‚’ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚
```
````

---
level: 2
layout: two-cols
transition: fade
---

# 5.1. encode_prompt

::right::

[<mdi-github-circle />pipeline.py#L41-L48](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L48)

```python {all|45|45-46}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
```

---
level: 2
layout: two-cols
transition: fade
---

# 5.1. encode_prompt

::right::

[<mdi-github-circle />pipeline.py#L41-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L57)

```python {all|54|54,56}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
 
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
layout: two-cols
transition: fade
---

# 5.1. encode_prompt

<v-clicks every="1" at="1">

- L54: `CLIPTokenizer`: ãƒ†ã‚­ã‚¹ãƒˆ(prompt)ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€‚ãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ã“ã¨ã§ã€AIã«æ‰±ã„ã‚„ã™ãã•ã›ã‚‹ã€‚

- L56: `CLIPTextModel`: è¨€èªã¨ç”»åƒã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚ç”»åƒç”Ÿæˆã«ãŠã„ã¦ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä½œã‚ŠãŸã„ç”»åƒã®è¡¨ç¾ï¼ˆembeddingï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L21-L39](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L21-L39)

```python {4,5|4|4,5}{lines:false,at:1}
@classmethod
def from_pretrained(cls, model_name, device=torch.device("cuda"), dtype=torch.float16):
	# Ommit comments
	tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")
	text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder")
	scheduler = PareDDIMScheduler.from_config(model_name, subfolder="scheduler")
	unet = PareUNet2DConditionModel.from_pretrained(model_name, subfolder="unet")
	vae = PareAutoencoderKL.from_pretrained(model_name, subfolder="vae")
	return cls(tokenizer, text_encoder, scheduler, unet, vae, device, dtype)
```

[<mdi-github-circle />pipeline.py#L50-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L50-L57)

```python {54,56|54|54,56}{lines:true,startLine:50,at:1}
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
layout: two-cols
transition: fade
---

# 5.1. encode_prompt


- L54: `CLIPTokenizer`: ãƒ†ã‚­ã‚¹ãƒˆ(prompt)ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€‚ãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ã“ã¨ã§ã€AIã«æ‰±ã„ã‚„ã™ãã•ã›ã‚‹ã€‚

- L56: `CLIPTextModel`: è¨€èªã¨ç”»åƒã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚ç”»åƒç”Ÿæˆã«ãŠã„ã¦ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä½œã‚ŠãŸã„ç”»åƒã®è¡¨ç¾ï¼ˆembeddingï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚


<v-clicks every="1" at="2">

- L46: ã‚·ãƒ³ãƒ—ãƒ«ã«ã™ã‚‹ãŸã‚ã«ã€negative_promptã¯ç©ºã®æ–‡å­—åˆ—ã¨ã—ã¦ã„ã¾ã™ã€‚

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L34-L35](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L34-L35)

```python {all}{lines:true,startLine:34,at:1}
	tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")
	text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder")
```

[<mdi-github-circle />pipeline.py#L41-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L57)

```python {|all|46|all}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
 
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
---

<iframe frameborder="0" scrolling="no" class="emg-iframe-text-inputs" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Funderstand-stable-diffusion-slidev-notebooks%2Fblob%2Fmain%2Fembed%2Fch5-text_inputs.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>


<style>
	.emg-iframe-text-inputs {
		transform: scale(0.9) translate(-50%, -50%); /* Apply both transformations */
		transform-origin: top left;
		position: absolute;
		top: 50%;
		left: 50%;
		width: 100%;
		height: 100%;
	}
</style>

---
level: 2
---

<iframe frameborder="0" scrolling="no" class="emg-iframe-prompt-embeds" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Funderstand-stable-diffusion-slidev-notebooks%2Fblob%2Fmain%2Fembed%2Fch5-prompt_embeds.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<style>
	.emg-iframe-prompt-embeds {
		transform: scale(0.8) translate(-50%, -50%); /* Apply both transformations */
		transform-origin: top left;
		position: absolute;
		top: 57%;
		left: 50%;
		width: 100%;
		height: 130%;
	}
</style>

---
level: 2
layout: center
---

<iframe frameborder="0" scrolling="yes" class="overflow-scroll emg-iframe-play-prompt-embeds" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fnotebooks%2Fch0.0.2_Play_prompt_embeds.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<style>
	.emg-iframe-play-prompt-embeds {
		transform: scale(0.5) translate(-50%, -50%); /* Apply both transformations */
		transform-origin: top left;
		position: absolute;
		top: 50%;
		left: 50%;
		width: 100%;
		height: 160%;
	}
</style>

<!--
ã¾ã‚‹ã§Word2Vec
-->

---
level: 2
layout: custom-two-cols
leftPercent: 0.3
---

# 5.2. get_latent

<v-clicks every="1">

- L63: 1/8ã®ã‚µã‚¤ã‚ºã®ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆ

<img src="/exps/latent.png" class="mt-5 h-48 object-contain" />

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L59-L65](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L59-L65)


```python {all|63|all}{lines:true,startLine:59,at:1,style:'--slidev-code-font-size: 1rem; --slidev-code-line-height: 1.5;'}
def get_latent(self, width: int, height: int):
	"""
	Generate a random initial latent tensor to start the diffusion process.
	"""
	return torch.randn((4, width // 8, height // 8)).to(
		device=self.device, dtype=self.dtype
	)
```

---
level: 2
layout: custom-two-cols
leftPercent: 0.5
transition: fade
---

# 5.3. denoise

<v-clicks every="1">

- L86: UNet

- L91: Scheduler

</v-clicks>

::right::


[<mdi-github-circle />pipeline.py#L75-L93](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L75-L93)

```python {all|86|86,91}{lines:true,startLine:75,at:1}
@torch.no_grad()
def denoise(self, latents, prompt_embeds, num_inference_steps=50, guidance_scale=7.5):
	"""
	Iteratively denoise the latent space using the diffusion model to produce an image.
	"""
	timesteps, num_inference_steps = self.retrieve_timesteps(num_inference_steps)

	for t in timesteps:
		latent_model_input = torch.cat([latents] * 2)
		
		# Predict the noise residual for the current timestep
		noise_residual = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)
		uncond_residual, text_cond_residual = noise_residual.chunk(2)
		guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

		# Update latents by reversing the diffusion process for the current timestep
		latents = self.scheduler.step(guided_noise_residual, t, latents)[0]

	return latents
```

---
level: 2
layout: custom-two-cols
leftPercent: 0.5
transition: fade
---

# 5.3. denoise

- L86: UNet2DConditionModel

- L91: DDIMScheduler


::right::

[<mdi-github-circle />pipeline.py#L21-L39](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L21-L39)

```python {6,7}{lines:false,at:1}
@classmethod
def from_pretrained(cls, model_name, device=torch.device("cuda"), dtype=torch.float16):
	# Ommit comments
	tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")
	text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder")
	scheduler = PareDDIMScheduler.from_config(model_name, subfolder="scheduler")
	unet = PareUNet2DConditionModel.from_pretrained(model_name, subfolder="unet")
	vae = PareAutoencoderKL.from_pretrained(model_name, subfolder="vae")
	return cls(tokenizer, text_encoder, scheduler, unet, vae, device, dtype)
```

[<mdi-github-circle />pipeline.py#L82-L93](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L82-L93)

```python {86,91}{lines:true,startLine:82,at:1}
	for t in timesteps:
		latent_model_input = torch.cat([latents] * 2)
		
		# Predict the noise residual for the current timestep
		noise_residual = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)
		uncond_residual, text_cond_residual = noise_residual.chunk(2)
		guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

		# Update latents by reversing the diffusion process for the current timestep
		latents = self.scheduler.step(guided_noise_residual, t, latents)[0]

	return latents
```

---
level: 2
layout: custom-two-cols
leftPercent: 0.5
---

# 5.3. denoise

<v-clicks every="1">

- L80: Schedulerã‚’ä½¿ã„timestepsã®å–å¾— <br />(<span class="text-sm">7. Schedulerã§è©³ã—ãèª¬æ˜</span>)

- L82: timestepsã®é•·ã•åˆ†ãƒ«ãƒ¼ãƒ—<br />(<span class="text-sm">timestepsã®é•·ã•åˆ† = num_inference_steps</span>)

- L86: UNetã§ãƒ‡ãƒã‚¤ã‚º <br />(<span class="text-sm">8. UNetã§è©³ã—ãèª¬æ˜</span>)

- L88: ã©ã‚Œã ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è€ƒæ…®ã™ã‚‹ã‹ã‚’è¨ˆç®— <br />(<span class="text-3">å‚è€ƒ: 
Jonathan Ho, Tim Salimans: â€œClassifier-Free Diffusion Guidanceâ€, 2022; <a href='http://arxiv.org/abs/2207.12598'>arXiv:2207.12598</a>.</span>)

- L91: Schedulerã«ã‚ˆã£ã¦ã€ãƒ‡ãƒã‚¤ã‚ºã®å¼·ã•ã‚’æ±ºå®š

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L82-L93](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L82-L93)

```python {all|80|82|86|88|91|all}{lines:true,startLine:75,at:1}
@torch.no_grad()
def denoise(self, latents, prompt_embeds, num_inference_steps=50, guidance_scale=7.5):
	"""
	Iteratively denoise the latent space using the diffusion model to produce an image.
	"""
	timesteps, num_inference_steps = self.retrieve_timesteps(num_inference_steps)

	for t in timesteps:
		latent_model_input = torch.cat([latents] * 2)
		
		# Predict the noise residual for the current timestep
		noise_residual = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)
		uncond_residual, text_cond_residual = noise_residual.chunk(2)
		guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

		# Update latents by reversing the diffusion process for the current timestep
		latents = self.scheduler.step(guided_noise_residual, t, latents)[0]

	return latents
```

---
level: 2
layout: center
---

# 5.3. denoise
Schedulerã¨UNetã‚’ä½¿ã†ã¨ã„ã†ã“ã¨ã ã‘è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚

::right::

[<mdi-github-circle />pipeline.py#L82-L93](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L82-L93)

```python {all|80|82|86|88|91|all}{lines:true,startLine:75,at:1}
@torch.no_grad()
def denoise(self, latents, prompt_embeds, num_inference_steps=50, guidance_scale=7.5):
	"""
	Iteratively denoise the latent space using the diffusion model to produce an image.
	"""
	timesteps, num_inference_steps = self.retrieve_timesteps(num_inference_steps)

	for t in timesteps:
		latent_model_input = torch.cat([latents] * 2)
		
		# Predict the noise residual for the current timestep
		noise_residual = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)
		uncond_residual, text_cond_residual = noise_residual.chunk(2)
		guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

		# Update latents by reversing the diffusion process for the current timestep
		latents = self.scheduler.step(guided_noise_residual, t, latents)[0]

	return latents
```

---
level: 2
layout: custom-two-cols
leftPercent: 0.4
---

# 5.4. vae_decode

<v-clicks every="1">
</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L107-L105](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L107-L115)

```python {all}{lines:true,startLine:107,at:1}
@torch.no_grad()
def vae_decode(self, latents):
	"""
	Decode the latent tensors using the VAE to produce an image.
	"""
	image = self.vae.decode(latents / self.vae.config.scaling_factor)[0]
	image = self.denormalize(image)
	image = self.tensor_to_image(image)
	return image
```

---
layout: cover
title: Scheduler
background: /backgrounds/scheduler.png
---

# 6. Scheduler

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Scheduler, flat vector illustration, best quality, high resolution</p>

---
level: 2
---

# <span class="text-3xl">[<mdi-github-circle />scheduler.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/scheduler.py)</span>
ãƒ‡ãƒã‚¤ã‚ºã®å¼·ã•ã‚’æ±ºå®š

<iframe frameborder="0" scrolling="yes" class="overflow-scroll iframe-full-code" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fscheduler.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

pipe.scheduler.step(guided_noise_residual, t, latents) ã‚’ã†ã¾ãä½¿ã£ã¦ã€schedulerãŒã©ã‚“ãªã“ã¨ã‚’ã—ã¦ã„ã‚‹ã‹ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œã‚ŠãŸã„ã€‚

---
layout: cover
title: UNet
background: /backgrounds/unet.png
---

# 7. UNet

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: UNet, watercolor painting, detailed, brush strokes, best quality, high resolution</p>

---
level: 2
---

# [<mdi-github-circle />unet.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/unet.py)
ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã«ä½¿ã‚ã‚Œã‚‹

<iframe frameborder="0" scrolling="yes" class="overflow-scroll iframe-full-code" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Funet.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
layout: image
image: /images/unet-figure.png
backgroundSize: 70%
class: 'text-black'
---

<!-- Reference -->
<p class="text-black text-xs abs-bl w-full mb-6 text-center">
Olaf Ronneberger, Philipp Fischer, Thomas Brox: â€œU-Net: Convolutional Networks for Biomedical Image Segmentationâ€, 2015; <a href='http://arxiv.org/abs/1505.04597'>arXiv:1505.04597</a>.
</p>

---
level: 2
layout: center
---

# 7.1 ãƒ¢ãƒ‡ãƒ«ä½œæˆ

```python
class PareUNet2DConditionModel(nn.Module):
	def __init__(self, **kwargs):
		super().__init__()
		self.config = DotDict(DEFAULT_UNET_CONFIG)
		self.config.update(kwargs)
		self.config.only_cross_attention = [self.config.only_cross_attention] * len(self.config.down_block_types)
		self.config.num_attention_heads = self.config.num_attention_heads or self.config.attention_head_dim
		self._setup_model_parameters()

		self._build_input_layers()
		self._build_time_embedding()
		self._build_down_blocks()
		self._build_mid_block()
		self._build_up_blocks()
		self._build_output_layers()
```

---
level: 2
---

Transformerä½¿ã£ã¦ã„ã‚‹ã“ã¨æ›¸ã?

---
level: 2
---

UNetã§æ»åœ¨ç©ºé–“ã‚’ä½œã£ã¦ã€å¹³å‡ã‚’å–ã‚Œã°ç‰¹å¾´ãŒæŠ½å‡ºã§ãã‚‹ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚‚ä½œã‚‹?

---
layout: cover
title: VAE
background: /backgrounds/vae.png
---

# 8. VAE

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: VAE, abstract style, highly detailed, colors and shapes</p>

---
level: 2
layout: center
---

# Variational Autoencoder
å¤‰åˆ†è‡ªå·±ç¬¦å·åŒ–å™¨ (æ—¥æœ¬èªè¨³ã‹ã£ã“ã„ã„!)

---
level: 2
---

# [<mdi-github-circle />vae.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/vae.py)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll iframe-full-code" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fvae.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

VAEã‚’å¤‰ãˆã¦ã‚‚ç”»åƒãŒç”Ÿæˆã§ãã‚‹è©±?
ç”»åƒç”Ÿæˆæ¨è«–ã‚µãƒ¼ãƒãƒ¼ã®VAEã‚’å¤‰æ›´æ©Ÿèƒ½è¿½åŠ ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§æœ€åˆã«ã‚„ã£ãŸã“ã¨ãªã®ã§ã¡ã‚‡ã£ã¨è©±ã›ã‚‹ã‹ã‚‚ã€‚

---
layout: cover
title: ã¾ã¨ã‚
background: /backgrounds/summary.png
---

# 9. ã¾ã¨ã‚

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Summary, long-exposure photography, masterpieces</p>

---
level: 2
layout: center
---

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’èª­ã‚€ã®æ¥½ã—ã„!

---
level: 2
layout: center
---

## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è‡³ã‚‹æ‰€ã§ã€è«–æ–‡ãŒå¼•ç”¨

[<mdi-github-circle />diffusers/.../pipeline.py](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py)

<img src="/images/diffusers-code-arxiv.png" class="mt-5 h-92 object-contain" />

---
level: 2
layout: center
---

ã¾ã¨ã‚
# 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
# 2. ãƒ©ãƒ³ãƒ€ãƒ ãªæ½œåœ¨ç©ºé–“ã®ç”Ÿæˆ
# 3. UNetã‚’ç”¨ã„ã¦ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°
# 4. Latent Spaceã‹ã‚‰Pixel Spaceã¸ã®ãƒ‡ã‚³ãƒ¼ãƒ‰

---
level: 2
layout: center
---

# ã”æ¸…è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼
