---
# try also 'default' to start simple
theme: seriph
colorSchema: 'light'

# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: backgrounds/understand-sd.png
# some information about your slides, markdown enabled
title: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç†è§£ã™ã‚‹Stable Diffusion
info: |
  ## Stable Diffusionã®ç”»åƒç”Ÿæˆã‚³ãƒ¼ãƒ‰ã®è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰
  diffusersã‚’ã‚·ãƒ³ãƒ—ãƒ«åŒ–ã—ãŸã€parediffusersã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’é€šã˜ã¦ã€Latent Diffusion Modelsã®ç”»åƒç”Ÿæˆã®ä»•çµ„ã¿ã‚’è§£èª¬ã™ã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã™ã€‚

author: masaishi
keywords: [ Stable Diffusion, Diffusers, parediffusers, AI, ML, Generative Models ]

export:
  format: pdf
  timeout: 30000
  dark: false
  withClicks: false

lineNumbers: true

# apply any unocss classes to the current slide
class: text-center
# https://sli.dev/custom/highlighters.html
highlighter: shiki
# https://sli.dev/guide/drawing
drawings:
  persist: true
# slide transition: https://sli.dev/guide/animations#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/guide/syntax#mdc-syntax
mdc: true

fonts:
  sans: Noto Serif JP, serif
---

# ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç†è§£ã™ã‚‹Stable Diffusion

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Understand Stable Diffusion from code, cyberpunk theme, best quality, high resolution, concept art</p>

---
title: è‡ªå·±ç´¹ä»‹
---

# 2. çŸ³åŸ æ­£å®— (Masamune Ishihara)
<div class="[&>*]:important-leading-10 opacity-80">
Computer Engineering Undergrad at University of California, Santa Cruz <br />
AI/MLã¨GISã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã€‚ <br />

<br />

#### å¥½ããªã‚‚ã®:
- ç´…èŒ¶
- ãƒ†ãƒ‹ã‚¹
- Rebuild.fm (<a href="https://rebuild.fm/223/" target="_blank" class="ml-1.5 border-none!">223: Ear Bleeding Pods (higepon)</a>ã‚’èã„ã¦kaggleã‚’å§‹ã‚ã¾ã—ãŸã€‚)
</div>

<div class="mt-10 flex flex-col gap-2">
  <div>
		<mdi-github-circle />
		<a href="https://github.com/masaishi" target="_blank" class="ml-1.5 border-none! font-300">masaishi</a>
	</div>
	<div>
		<mdi-twitter />
		<a href="https://twitter.com/masaishi2001" target="_blank" class="ml-1.5 border-none! font-300">@masaishi2001</a>
	</div>
	<div>
		<mdi-linkedin />
		<a href="https://www.linkedin.com/in/masamune-ishihara" target="_blank" class="ml-1.5 border-none! font-300">masamune-ishihara</a>
	</div>
	<div class="flex items-center">
		<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512" class="h-5 w-5"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M304.2 501.5L158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.3 0 6-3z"/></svg>
		<a href="https://www.kaggle.com/masaishi" target="_blank" class="ml-1.5 border-none! font-300">masaishi</a>
	</div>
</div>

<img src="/images/icon_tea_light.png" class="rounded-full w-35 abs-tr mt-12 mr-24" />

---
level: 2
layout: center
---

# Kagglerã«ã¨ã£ã¦ç”»åƒç”Ÿæˆã‚’ä½¿ã†æ©Ÿä¼šã¯å°‘ãªã„?

<img src="/images/stable-diffusion-image-to-prompts.png" class="h-100" />

<a src="https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/overview" target="_blank" class="abs-bl w-full mb-6 text-center text-xs text-black border-none!">https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/overview</a>

---
level: 2
layout: center
---

ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç›®çš„

# ç”»åƒç”Ÿæˆã®ã‚³ãƒ¼ãƒ‰ã‚’1é€šã‚Šç´¹ä»‹ã—ãŸã„ã€‚

---
level: 2
layout: center
---

# ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¤ã„ã¦
ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’æ›¸ãã«ã‚ãŸã£ã¦ã€è‡ªåˆ†ãŒç†è§£ã§ãã¦ã„ãªã‹ã£ãŸã¨ã“ã‚ãªã©ã‚’å¤šãå®Ÿæ„Ÿã—ã¾ã—ãŸã€‚é–“é•ã£ãŸèª¬æ˜ã‚’ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã€ä¸æ˜ç­ãªã¨ã“ã‚ã‚„ã€é–“é•ã„ã‚’ä¸‹è¨˜ã®ãƒªãƒ³ã‚¯ã‹ã‚‰æ•™ãˆã¦ã„ãŸã ã‘ã‚‹ã¨å¹¸ã„ã§ã™ã€‚

<br />

### [<mdi-github-circle />understand-stable-diffusion-slidev-ja](https://github.com/masaishi/understand-stable-diffusion-slidev-ja)

<br />

[<mdi-radiobox-marked />Issues](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/issues): é–“é•ã„ã‚’è¦‹ã¤ã‘ãŸã‚‰ã”æŒ‡æ‘˜ãã ã•ã„ã€‚
<br />

[<mdi-message-text-outline />Discussions](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/discussions): è³ªå•ãŒã‚ã‚Œã°ã“ã¡ã‚‰ã‹ã‚‰ãŠé¡˜ã„ã—ã¾ã™ã€‚
<br />

[<mdi-source-pull />Pull Requests](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/pulls): ã‚‚ã—ä¿®æ­£ãŒã‚ã‚Œã°ãŠé€ã‚Šãã ã•ã„ã€‚
<br />

---
layout: center
title: ç›®æ¬¡
---

# ç›®æ¬¡
<Toc minDepth="1" maxDepth="1"></Toc>

---
layout: cover
title: Stable Diffusionã®æ¦‚è¦
background: /backgrounds/stable-diffusion.png
---

# 4. Stable Diffusionã®æ¦‚è¦

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Stable Diffusion, watercolor painting, best quality, high resolution</p>

---
level: 2
layout: center
---

# Stable Diffusionã¨ã¯?

- StabilityAIã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸLatent Diffusion Model (LDM)ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«
- Text-to-Image, Image-to-Imageãªã©ã®ã‚¿ã‚¹ã‚¯ã«åˆ©ç”¨å¯èƒ½
- Diffusersã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ç°¡å˜ã«å‹•ã‹ã™ã“ã¨ãŒã§ãã‚‹ã€‚
- https://arxiv.org/abs/2112.10752

---
level: 2
layout: center
---

Latent Diffusion Model (LDM)ã¨ã¯?

# Denoising Diffusion Probabilistic Model (DDPM) ã« Latent Space(æ»åœ¨ç©ºé–“)ã¨ã„ã†æ¦‚å¿µã‚’è¿½åŠ ã—ãŸä»•çµ„ã¿

---
level: 2
---

Denoising Diffusion Probabilistic Model (DDPM)ã¨ã¯?

<h1 class="!text-7.9">ç”»åƒã«<span v-mark.red="1">ãƒã‚¤ã‚ºã‚’åŠ ãˆ</span>ã€ãã“ã‹ã‚‰<span v-mark.blue="2">å…ƒã®ç”»åƒã«å¾©å…ƒ</span>ã™ã‚‹ãƒ¢ãƒ‡ãƒ«</h1>

<p>éŸ³å£°ãªã©ã®ãƒ‡ãƒ¼ã‚¿å…¨èˆ¬ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã¯ç”»åƒã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚</p>

<ul>
	<li><span v-mark.red="1">Diffusion process(æ‹¡æ•£éç¨‹)</span>ã‚’ç”¨ã„ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’è¡Œã†ã€‚ç¢ºç‡éç¨‹ï¼ˆç‰¹ã«ãƒãƒ«ã‚³ãƒ•é€£é–)</li>
	<li><span v-mark.blue="2">Reverse process(é€†æ‹¡æ•£éç¨‹)</span>ã‚’ç”¨ã„ã€ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒã™ã‚‹ã€‚</li>
</ul>

<br />

<img src="/images/ddpm-figure.png" class="abs-b mb-10 ml-auto mr-auto w-5/6" />

<!-- Reference -->
<p class="text-black text-xs abs-bl w-full mb-6 text-center">
Jonathan Ho, Ajay Jain, Pieter Abbeel: â€œDenoising Diffusion Probabilistic Modelsâ€, 2020; <a href='http://arxiv.org/abs/2006.11239'>arXiv:2006.11239</a>.
</p>

---
level: 2
layout: center
---

Latent Diffusion Model (LDM)ã¨ã¯?

# Latent(æ»åœ¨)ç©ºé–“ã§ã€Denoising Diffusion Probabilistic Model (DDPM) ã‚’è¨ˆç®—ã™ã‚‹ä»•çµ„ã¿

---
level: 2
---

<div class="flex flex-col !justify-between w-full h-120">
	<div>
		<img src="/images/ddpm-figure.png" class="ml-auto mr-auto h-26" />
		<!-- Reference -->
		<p class="text-black text-xs w-full mt-6 text-center">
		Jonathan Ho, Ajay Jain, Pieter Abbeel: â€œDenoising Diffusion Probabilistic Modelsâ€, 2020; <a href='http://arxiv.org/abs/2006.11239'>arXiv:2006.11239</a>.
		</p>
	</div>
	<div v-click>
		<img src="/images/stable-diffusion-figure.png" alt="Stable Diffusion Figure" class="ml-auto mr-auto h-48 object-contain" />
		<p class="text-black text-xs w-full mt-6 text-center">
		Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer: â€œHigh-Resolution Image Synthesis with Latent Diffusion Modelsâ€, 2021; <a href='http://arxiv.org/abs/2112.10752'>arXiv:2112.10752</a>.
		</p>
	</div>
</div>

---
level: 2
layout: center
---

Latent Diffusion Model (LDM)ã¨ã¯?

# Latent(æ»åœ¨)ç©ºé–“ã§ã€Denoising Diffusion Probabilistic Model (DDPM) ã‚’è¨ˆç®—ã™ã‚‹ä»•çµ„ã¿

---
level: 2
layout: center
---

Latent Space(æ»åœ¨ç©ºé–“)ã¨ã¯?

# å…¥åŠ›ç”»åƒã®ç‰¹å¾´ã‚’æŠ½å‡ºã—ãŸç©ºé–“ 

TODO: VAEã‚’é€šã—ãŸç”»åƒã®å¹³å‡ã‚’ã¨ã£ãŸç”»åƒã‚’ç”¨æ„ã™ã‚‹ã€‚

---
level: 2
layout: center
---

ã–ã£ãã‚Šã—ãŸèª¬æ˜

<h1>ãƒ©ãƒ³ãƒ€ãƒ ãªLatentã‚’ä½œã‚‹<br />
UNetã§ã€ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã‚’è¡Œã†<br />
VAEã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã€ç”»åƒã‚’ç”Ÿæˆã™ã‚‹</h1>

---
level: 2
layout: center
---

# Diffusersã¨ã¯?

- Hugging FaceğŸ¤—ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸDiffusion Modelsã‚’æ‰±ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«å‹•ã‹ã™ã“ã¨ãŒã§ãã‚‹ã€‚
- <mdi-github-circle /> https://github.com/huggingface/diffusers

---
level: 2
layout: image-right
image: /exps/d-sd2-sample-42.png
---

# [<mdi-github-circle />Diffusers](https://github.com/huggingface/diffusers)ã‚’è©¦ã™
## <!-- TODO: Find better way, currently for avoide below becomes subtitle -->

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1EbqeoWL5kPaDA8INLWl8g34v3vn83AQ5?usp=sharing)

Install the Diffusers library:
```python
!pip install transformers diffusers accelerate -U
```

Generate an image from text:
```python {all|4-7|8|10|all}{lines:true}
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
  "stabilityai/stable-diffusion-2",
  dtype=torch.float16,
).to(device=torch.device("cuda"))
prompt = "painting depicting the sea, sunrise, ship, artstation, 4k, concept art"

image = pipe(prompt, width=512, height=512).images[0]
display(image)
```

---
level: 2
layout: center
---

# Diffusersã¯æ©Ÿèƒ½ãŒè±Šå¯Œã§æŸ”è»Ÿæ€§ã‚‚é«˜ã„ãŒã€<br />
# ãã®åˆ†ã‚³ãƒ¼ãƒ‰ã®ç†è§£ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã€‚

---
level: 2
---

# [<mdi-github-circle />diffusers/.../pipeline_stable_diffusion.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll mt-10" style="width:100%; height:85%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fdiffusers%2Fblob%2Fmain%2Fsrc%2Fdiffusers%2Fpipelines%2Fstable_diffusion%2Fpipeline_stable_diffusion.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

# [<mdi-github-circle />parediffusers/.../pipeline.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/pipeline.py)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll mt-10" style="width:100%; height:85%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fpipeline.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
layout: image-right
image: /exps/p-sd2-sample-43.png
---

# [<mdi-github-circle />PareDiffusers](https://github.com/masaishi/parediffusers)
## <!-- TODO: Find better way, currently for avoide below becomes subtitle -->

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1I-qU3hfF19T42ksIh5FC0ReyKZ2hsJvx?usp=sharing)

Install the PareDiffusers library:
```python
!pip install parediffusers
```

Generate an image from text:
```python {all|2|4|6}{lines:true}
import torch
from parediffusers import PareDiffusionPipeline

pipe = PareDiffusionPipeline.from_pretrained(
  "stabilityai/stable-diffusion-2",
  device=torch.device("cuda"),
  dtype=torch.float16,
)
prompt = "painting depicting the sea, sunrise, ship, artstation, 4k, concept art"

image = pipe(prompt, width=512, height=512)
display(image)
```

---
layout: cover
title: Pipeline
background: /backgrounds/pipeline.png
---

# 5. Pipeline

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Pipeline, cyberpunk theme, best quality, high resolution, concept art</p>

---
level: 2
layout: image
image: /images/stable-diffusion-figure.png
backgroundSize: 70%
class: 'text-black'
---

<!-- Reference -->
<p class="text-black text-xs abs-bl w-full mb-6 text-center">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer: â€œHigh-Resolution Image Synthesis with Latent Diffusion Modelsâ€, 2021; <a href='http://arxiv.org/abs/2112.10752'>arXiv:2112.10752</a>.
</p>

---
level: 2
layout: center
---

<iframe frameborder="0" scrolling="no" style="width:100%; height:163px;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2F035772c684ae8d16c7c908f185f6413b72658126%2Fsrc%2Fparediffusers%2Fpipeline.py%23L131-L134&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<div class="w-full flex flex-col justify-center mt-10">
<img src="/images/stable-diffusion-figure.png" alt="Stable Diffusion Figure" class="h-48 object-contain" />
<p class="text-black text-xs w-full mt-6 text-center">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer: â€œHigh-Resolution Image Synthesis with Latent Diffusion Modelsâ€, 2021; <a href='http://arxiv.org/abs/2112.10752'>arXiv:2112.10752</a>.
</p>
</div>

---
level: 2
layout: center
---

````md magic-move
```python {all}{lines:true}
prompt_embeds = self.encode_prompt(prompt)
latents = self.get_latent(width, height).unsqueeze(dim=0)
latents = self.denoise(latents, prompt_embeds, num_inference_steps, guidance_scale)
image = self.vae_decode(latents)
```
```md {all|1|2|3|4|all}
1. `encode_prompt` : ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’embeddingã«å¤‰æ›ã™ã‚‹ã€‚
2. `get_latent` : ç”Ÿæˆã—ãŸã„ç”»åƒã‚µã‚¤ã‚ºã®ã€1/8ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
3. `denoise` : ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®embeddingã‹ã‚‰ã€æ½œåœ¨ç©ºé–“ã‚’åå¾©çš„ã«ãƒ‡ãƒã‚¤ã‚ºã™ã‚‹ã€‚
4. `vae_decode` : ãƒ‡ãƒã‚¤ã‚ºã•ã‚ŒãŸæ½œåœ¨ç©ºé–“ã‚’ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚
```
````

---
level: 2
layout: two-cols
transition: fade
---

# 5.1. encode_prompt

::right::

[<mdi-github-circle />pipeline.py#L41-L48](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L48)

```python {all|45|45-46}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
```

---
level: 2
layout: two-cols
transition: fade
---

# 5.1. encode_prompt

::right::

[<mdi-github-circle />pipeline.py#L41-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L57)

```python {all|54|54,56}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
 
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
layout: two-cols
---

# 5.1. encode_prompt

<v-clicks every="1" at="1">

- L34: `CLIPTokenizer`: ãƒ†ã‚­ã‚¹ãƒˆ(prompt)ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€‚ãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ã“ã¨ã§ã€AIã«æ‰±ã„ã‚„ã™ãã•ã›ã‚‹ã€‚

- L35: `CLIPTextModel`: è¨€èªã¨ç”»åƒã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚ç”»åƒç”Ÿæˆã«ãŠã„ã¦ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä½œã‚ŠãŸã„ç”»åƒã®è¡¨ç¾ï¼ˆembeddingï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L21-L39](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L21-L39)

```python {all|4|5|4,5}{lines:false,at:1}
@classmethod
def from_pretrained(cls, model_name, device=torch.device("cuda"), dtype=torch.float16):
	# Ommit comments
	tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")
	text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder")
	scheduler = PareDDIMScheduler.from_config(model_name, subfolder="scheduler")
	unet = PareUNet2DConditionModel.from_pretrained(model_name, subfolder="unet")
	vae = PareAutoencoderKL.from_pretrained(model_name, subfolder="vae")
	return cls(tokenizer, text_encoder, scheduler, unet, vae, device, dtype)
```

[<mdi-github-circle />pipeline.py#L50-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L50-L57)

```python {all|54|56|54,56}{lines:true,startLine:50,at:1}
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
---

<iframe frameborder="0" scrolling="no" class="emg-iframe-text-inputs" style="width:90%; height:90%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Funderstand-stable-diffusion-slidev-notebooks%2Fblob%2Fmain%2Fembed%2Fch5-text_inputs.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<style>
	.emg-iframe-text-inputs {
		/* apply the transform */
		/*--scale-factor: 0.1;*/
		-webkit-transform:scale(0.6);
		-moz-transform:scale(0.6);
		-o-transform:scale(0.6);
		transform:scale(0.6);
		/* position it, as if it was the original size */
		position: absolute;
		top: 50%;
		left: 50%;
		transform: translate(-50%, -50%);
	}
</style>

---
level: 2
---

<iframe frameborder="0" scrolling="no" class="emg-iframe-prompt-embeds" style="width:80%; height:110%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Funderstand-stable-diffusion-slidev-notebooks%2Fblob%2Fmain%2Fembed%2Fch5-prompt_embeds.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<style>
	.emg-iframe-prompt-embeds {
		-ms-zoom: 0.65;
		-moz-transform: scale(0.65);
		-moz-transform-origin: 0 0;
		-o-transform: scale(0.65);
		-o-transform-origin: 0 0;
		-webkit-transform: scale(0.65);
		-webkit-transform-origin: 0 0;

		position: absolute;
		top: 50%;
		left: 50%;
		transform: translate(-50%, -50%);
	}
</style>

---
level: 2
layout: two-cols
---

# 5.1. encode_prompt

<v-clicks every="1" at="1">

- L45: `get_embes`é–¢æ•°ã‚’å‘¼ã³prompt_embedsã‚’å–å¾—

- L46: `get_embes`é–¢æ•°ã‚’å‘¼ã³negative_prompt_embedsã‚’å–å¾— (ã‚·ãƒ³ãƒ—ãƒ«ã«ã™ã‚‹ãŸã‚ã«ã€negative_promptã¯ç©ºã®æ–‡å­—åˆ—ã¨ã—ã¦ã„ã¾ã™ã€‚)

- L54: CLIPTokenizerã§Tokenize

- L56: CLIPTextModelã§embeddingã‚’å–å¾—

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L34-L35](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L34-L35)

```python {all|none|none|34|35|all}{lines:true,startLine:34,at:1}
	tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")
	text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder")
```

[<mdi-github-circle />pipeline.py#L41-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L57)

```python {all|45|46|54|56|all}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
 
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
---

<iframe frameborder="0" scrolling="yes" class="overflow-scroll mt-10" style="width:100%; height:90%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fnotebooks%2Fch0.0.2_Play_prompt_embeds.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
layout: custom-two-cols
leftPercent: 0.4
---

# 5.2. get_latent

<v-clicks every="1">

- L63: 1/8ã®ã‚µã‚¤ã‚ºã®ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆ

<img src="/exps/latent.png" class="mt-5 h-48 object-contain" />

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L59-L65](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L59-L65)


```python {all|63|all}{lines:true,startLine:59,at:1}
def get_latent(self, width: int, height: int):
	"""
	Generate a random initial latent tensor to start the diffusion process.
	"""
	return torch.randn((4, width // 8, height // 8)).to(
		device=self.device, dtype=self.dtype
	)
```

---
level: 2
layout: custom-two-cols
leftPercent: 0.4
---

# 5.3. denoise

::right::

[<mdi-github-circle />pipeline.py#L75-L93](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L75-L93)

```python {all}
@torch.no_grad()
def denoise(self, latents, prompt_embeds, num_inference_steps=50, guidance_scale=7.5):
	"""
	Iteratively denoise the latent space using the diffusion model to produce an image.
	"""
	timesteps, num_inference_steps = self.retrieve_timesteps(num_inference_steps)

	for t in timesteps:
		latent_model_input = torch.cat([latents] * 2)
		
		# Predict the noise residual for the current timestep
		noise_residual = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)
		uncond_residual, text_cond_residual = noise_residual.chunk(2)
		guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

		# Update latents by reversing the diffusion process for the current timestep
		latents = self.scheduler.step(guided_noise_residual, t, latents)[0]

	return latents
```

---
level: 2
layout: two-cols
---

# 5.4. vae_decode

---
level: 2
---

# ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ

````md magic-move
```bash
parediffusers
â”œâ”€â”€ __init__.py
â”œâ”€â”€ defaults.py
â”œâ”€â”€ models
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ attension.py
â”‚Â Â  â”œâ”€â”€ embeddings.py
â”‚Â Â  â”œâ”€â”€ resnet.py
â”‚Â Â  â”œâ”€â”€ transformer.py
â”‚Â Â  â”œâ”€â”€ transformer_blocks.py
â”‚Â Â  â”œâ”€â”€ unet_2d_blocks.py
â”‚Â Â  â”œâ”€â”€ unet_2d_get_blocks.py
â”‚Â Â  â”œâ”€â”€ unet_2d_mid_blocks.py
â”‚Â Â  â””â”€â”€ vae_blocks.py
â”œâ”€â”€ pipeline.py
â”œâ”€â”€ scheduler.py
â”œâ”€â”€ unet.py
â”œâ”€â”€ utils.py
â””â”€â”€ vae.py
```
```bash
parediffusers
â”œâ”€â”€ __init__.py 
â”œâ”€â”€ defaults.py
â”œâ”€â”€ models # UNetã‚„VAEã®æ§‹ç¯‰ã®ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ attension.py
â”‚Â Â  â”œâ”€â”€ embeddings.py
â”‚Â Â  â”œâ”€â”€ resnet.py
â”‚Â Â  â”œâ”€â”€ transformer.py
â”‚Â Â  â”œâ”€â”€ transformer_blocks.py
â”‚Â Â  â”œâ”€â”€ unet_2d_blocks.py
â”‚Â Â  â”œâ”€â”€ unet_2d_get_blocks.py
â”‚Â Â  â”œâ”€â”€ unet_2d_mid_blocks.py
â”‚Â Â  â””â”€â”€ vae_blocks.py
â”œâ”€â”€ pipeline.py # ç”»åƒç”Ÿæˆã®ãŸã‚ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ 5. Pipelineã§è©³ã—ãèª¬æ˜
â”œâ”€â”€ scheduler.py # DDIMSchedulerã®å®Ÿè£… 4. Schedulerã§è©³ã—ãèª¬æ˜
â”œâ”€â”€ unet.py # UNet2DConditionModelã®å®Ÿè£… 6. UNetã§è©³ã—ãèª¬æ˜
â”œâ”€â”€ utils.py # æ´»æ€§åŒ–é–¢æ•°ãªã©ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
â””â”€â”€ vae.py # AutoencoderKLã®å®Ÿè£… 8. VAEã§è©³ã—ãèª¬æ˜
```
````

---
level: 2
---

# defaults.py

<iframe frameborder="0" scrolling="yes" class="overflow-scroll" style="width:100%; height:90%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fdefaults.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

# models/

- `attention.py`: TransformerBlockã‚„Unetã§ä½¿ã‚ã‚Œã‚‹Attentionãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®Ÿè£…
- `embeddings.py`: UNetã§ä½¿ã‚ã‚Œã‚‹Timestepsãªã©ã®å®Ÿè£…
- `resnet.py`: UNetã§ä½¿ã‚ã‚Œã‚‹ResNetãªã©ã®å®Ÿè£…
- `transformer.py`: UNetã§ä½¿ã‚ã‚Œã‚‹Transformerã®å®Ÿè£…
- `transformer_blocks.py`: Transformerã«ä½¿ã‚ã‚Œã‚‹TransformerBlockã®å®Ÿè£…
- `unet_2d_blocks.py`: get_unet_2d_blocksã§ä½¿ã‚ã‚Œã‚‹UNetBlockã®å®Ÿè£…
- `unet_2d_get_blocks.py`: UNetã‚„VAEã®Encoderã‚„Decoderã§ä½¿ã‚ã‚Œã‚‹get_up_blockã‚„get_down_blockã®å®Ÿè£…
- `unet_2d_mid_blocks.py`: UNetã§ä½¿ã‚ã‚Œã‚‹UNetMidBlockã®å®Ÿè£…
- `vae_blocks.py`: VAEã§ä½¿ã‚ã‚Œã‚‹VAEBlockã®å®Ÿè£…

---
level: 2
---

# pipeline.py

<iframe frameborder="0" scrolling="yes" class="overflow-scroll" style="width:100%; height:90%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fpipeline.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

# scheduler.py (6. ã§è©³ã—ãèª¬æ˜)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll" style="width:100%; height:90%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fscheduler.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

# unet.py (7. ã§è©³ã—ãèª¬æ˜)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll" style="width:100%; height:90%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Funet.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

# utils.py

<iframe frameborder="0" scrolling="yes" class="overflow-scroll" style="width:100%; height:90%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Futils.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

# vae.py (8. ã§è©³ã—ãèª¬æ˜)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll" style="width:100%; height:90%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fvae.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
layout: cover
title: Scheduler
background: /backgrounds/scheduler.png
---

# 6. Scheduler

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Scheduler, flat vector illustration, best quality, high resolution</p>

---
level: 2
---

pipe.scheduler.step(guided_noise_residual, t, latents) ã‚’ã†ã¾ãä½¿ã£ã¦ã€schedulerãŒã©ã‚“ãªã“ã¨ã‚’ã—ã¦ã„ã‚‹ã‹ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œã‚ŠãŸã„ã€‚

---
level: 2
layout: image-right
image: /exps/skip-scheduler-result.png
---

5å€‹é£›ã°ã—ã§ã‚‚ç”»åƒã¯ç”Ÿæˆã§ãã‚‹ã®ã‹?

```python
timesteps, num_inference_steps = pare_pipe.retrieve_timesteps(50)
timesteps = timesteps[::5]

guidance_scale = 7.5

for i, t in enumerate(timesteps):
	latent_model_input = torch.cat([latents] * 2)
	
	# Predict the noise residual for the current timestep
	noise_residual = pare_pipe.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)[0]
	uncond_residual, text_cond_residual = noise_residual.chunk(2)
	guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

	# Update latents by reversing the diffusion process for the current timestep
	latents = pare_pipe.scheduler.step(guided_noise_residual, t, latents)[0]

image = pare_pipe.vae_decode(latents)
display(image)
```

---
layout: cover
title: UNet
background: /backgrounds/unet.png
---

# 7. UNet

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: UNet, watercolor painting, detailed, brush strokes, best quality, high resolution</p>

---
level: 2
---

UNetã®æ§‹é€ ã®å›³

---

Transformerä½¿ã£ã¦ã„ã‚‹ã“ã¨æ›¸ã?

---

UNetã§æ»åœ¨ç©ºé–“ã‚’ä½œã£ã¦ã€å¹³å‡ã‚’å–ã‚Œã°ç‰¹å¾´ãŒæŠ½å‡ºã§ãã‚‹ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚‚ä½œã‚‹?

---
layout: cover
title: VAE
background: /backgrounds/vae.png
---

# 8. VAE

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: VAE, abstract style, highly detailed, colors and shapes</p>

---

VAEã‚’å¤‰ãˆã¦ã‚‚ç”»åƒãŒç”Ÿæˆã§ãã‚‹è©±?
ç”»åƒç”Ÿæˆæ¨è«–ã‚µãƒ¼ãƒãƒ¼ã®VAEã‚’å¤‰æ›´æ©Ÿèƒ½è¿½åŠ ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§æœ€åˆã«ã‚„ã£ãŸã“ã¨ãªã®ã§ã¡ã‚‡ã£ã¨è©±ã›ã‚‹ã‹ã‚‚ã€‚

---
layout: cover
title: ã¾ã¨ã‚
background: /backgrounds/summary.png
---

# 9. ã¾ã¨ã‚

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Summary, long-exposure photography, masterpieces</p>
