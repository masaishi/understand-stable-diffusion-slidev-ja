---
# try also 'default' to start simple
theme: seriph
colorSchema: 'light'

# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: backgrounds/understand-sd.webp
# some information about your slides, markdown enabled
title: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç†è§£ã™ã‚‹Stable Diffusion
info: |
  ## Stable Diffusionã®ç”»åƒç”Ÿæˆã‚³ãƒ¼ãƒ‰ã®è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰
  diffusersã‚’ã‚·ãƒ³ãƒ—ãƒ«åŒ–ã—ãŸã€parediffusersã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’é€šã˜ã¦ã€Latent Diffusion Modelsã®ç”»åƒç”Ÿæˆã®ä»•çµ„ã¿ã‚’è§£èª¬ã™ã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã™ã€‚

author: masaishi
keywords: [ Stable Diffusion, Diffusers, parediffusers, AI, ML, Generative Models ]

export:
  format: pdf
  timeout: 30000
  dark: false
  withClicks: false

lineNumbers: true

# apply any unocss classes to the current slide
class: text-center
# https://sli.dev/custom/highlighters.html
highlighter: shiki
# https://sli.dev/guide/drawing
drawings:
  persist: true
# slide transition: https://sli.dev/guide/animations#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/guide/syntax#mdc-syntax
mdc: true

fonts:
  sans: Noto Serif JP, serif
---

# ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç†è§£ã™ã‚‹Stable Diffusion

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Understand Stable Diffusion from code, cyberpunk theme, best quality, high resolution, concept art</p>

---
title: è‡ªå·±ç´¹ä»‹
---

# 2. çŸ³åŸ æ­£å®— (Masamune Ishihara)
<div class="[&>*]:important-leading-10 opacity-80">
Computer Engineering Undergrad at University of California, Santa Cruz <br />
AI/MLã¨GISã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™ã€‚ <br />

<br />

#### å¥½ããªã‚‚ã®:
- ç´…èŒ¶
- ãƒ†ãƒ‹ã‚¹
- Rebuild.fm (<a href="https://rebuild.fm/223/" target="_blank" class="ml-1.5 border-none!">223: Ear Bleeding Pods (higepon)</a>ã‚’èã„ã¦kaggleã‚’å§‹ã‚ã¾ã—ãŸã€‚)
</div>

<div class="mt-10 flex flex-col gap-2">
  <div>
		<mdi-github-circle />
		<a href="https://github.com/masaishi" target="_blank" class="ml-1.5 border-none! font-300">masaishi</a>
	</div>
	<div>
		<mdi-twitter />
		<a href="https://twitter.com/masaishi2001" target="_blank" class="ml-1.5 border-none! font-300">@masaishi2001</a>
	</div>
	<div>
		<mdi-linkedin />
		<a href="https://www.linkedin.com/in/masamune-ishihara" target="_blank" class="ml-1.5 border-none! font-300">masamune-ishihara</a>
	</div>
	<div class="flex items-center">
		<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512" class="h-5 w-5"><!--!Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M304.2 501.5L158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.3 0 6-3z"/></svg>
		<a href="https://www.kaggle.com/masaishi" target="_blank" class="ml-1.5 border-none! font-300">masaishi</a>
	</div>
</div>

<img src="/images/icon_tea_light.webp" class="rounded-full w-35 abs-tr mt-12 mr-24" />

---
level: 2
layout: center
---

# Kagglerã«ã¨ã£ã¦ç”»åƒç”Ÿæˆã‚’ä½¿ã†æ©Ÿä¼šã¯å°‘ãªã„?

<img src="/images/stable-diffusion-image-to-prompts.webp" class="h-100" />

<a src="https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/overview" target="_blank" class="abs-bl w-full mb-6 text-center text-xs text-black border-none!">https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/overview</a>

---
level: 2
layout: center
---

ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®ç›®çš„

# ç”»åƒç”Ÿæˆã®æµã‚Œã‚’ã‚³ãƒ¼ãƒ‰ã¨ä¸€ç·’ã«ç´¹ä»‹ã—ãŸã„

---
level: 2
layout: center
transition: fade
---

# ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¤ã„ã¦
ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’æ›¸ãã«ã‚ãŸã£ã¦ã€è‡ªåˆ†ãŒç†è§£ã§ãã¦ã„ãªã‹ã£ãŸã¨ã“ã‚ãªã©ã‚’å¤šãå®Ÿæ„Ÿã—ã¾ã—ãŸã€‚é–“é•ã£ãŸèª¬æ˜ã‚’ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§ã€ä¸æ˜ç­ãªã¨ã“ã‚ã‚„ã€é–“é•ã„ã‚’ä¸‹è¨˜ã®ãƒªãƒ³ã‚¯ã‹ã‚‰æ•™ãˆã¦ã„ãŸã ã‘ã‚‹ã¨å¹¸ã„ã§ã™ã€‚

<br />

### [<mdi-github-circle />understand-stable-diffusion-slidev-ja](https://github.com/masaishi/understand-stable-diffusion-slidev-ja)

<br />

[<mdi-radiobox-marked />Issues](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/issues): é–“é•ã„ã‚’è¦‹ã¤ã‘ãŸã‚‰ã”æŒ‡æ‘˜ãã ã•ã„ã€‚
<br />

[<mdi-message-text-outline />Discussions](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/discussions): è³ªå•ãŒã‚ã‚Œã°ã“ã¡ã‚‰ã‹ã‚‰ãŠé¡˜ã„ã—ã¾ã™ã€‚
<br />

[<mdi-source-pull />Pull Requests](https://github.com/masaishi/understand-stable-diffusion-slidev-ja/pulls): ã‚‚ã—ä¿®æ­£ãŒã‚ã‚Œã°ãŠé€ã‚Šãã ã•ã„ã€‚
<br />

---
level: 2
layout: center
---

# ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¤ã„ã¦
ç”»åƒç”Ÿæˆã®æµã‚Œã‚’ã‚³ãƒ¼ãƒ‰ã¨ä¸€ç·’ã«ç´¹ä»‹ã¨ã„ã†ã‚³ãƒ³ã‚»ãƒ—ãƒˆã®ãŸã‚ã€åŸºæœ¬çš„ã«è¼‰ã›ã¦ã„ã‚‹å…¨ã¦ã®ã‚³ãƒ¼ãƒ‰ã¯ã€å®Ÿéš›ã«å‹•ã‹ã™ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚

<br />

### ãƒ¬ãƒã‚¸ãƒˆãƒªä¸€è¦§

[<mdi-github-circle />understand-stable-diffusion-slidev-ja](https://github.com/masaishi/understand-stable-diffusion-slidev-ja): ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒ¬ãƒã‚¸ãƒˆãƒª

[<mdi-github-circle />understand-stable-diffusion-slidev-notebooks](https://github.com/masaishi/understand-stable-diffusion-slidev-notebooks): ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚„ã€gifã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯

[<mdi-github-circle />parediffusers](https://github.com/masaishi/parediffusers): ãƒ¡ã‚¤ãƒ³ã§æ‰±ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

---
layout: center
title: ç›®æ¬¡
---

# ç›®æ¬¡
<Toc minDepth="1" maxDepth="1"></Toc>

---
layout: cover
title: ç”»åƒç”Ÿæˆã®æµã‚Œ
background: /backgrounds/stable-diffusion.webp
---

# 4. ç”»åƒç”Ÿæˆã®æµã‚Œ

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Stable Diffusion, watercolor painting, best quality, high resolution</p>

---
level: 2
layout: center
---

# Stable Diffusionã¨ã¯?

- StabilityAIã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸLatent Diffusion Model (LDM)ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«
- Text-to-Image, Image-to-Imageãªã©ã®ã‚¿ã‚¹ã‚¯ã«åˆ©ç”¨å¯èƒ½
- Diffusersã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ç°¡å˜ã«å‹•ã‹ã™ã“ã¨ãŒã§ãã‚‹ã€‚
- https://arxiv.org/abs/2112.10752

---
level: 2
layout: center
---

# Diffusersã¨ã¯?

- Hugging FaceğŸ¤—ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸDiffusion Modelsã‚’æ‰±ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«å‹•ã‹ã™ã“ã¨ãŒã§ãã‚‹ã€‚
- <mdi-github-circle /> https://github.com/huggingface/diffusers

---
level: 2
layout: image-right
image: /exps/d-sd2-sample-42.webp
---

# [<mdi-github-circle />Diffusers](https://github.com/huggingface/diffusers)ã‚’è©¦ã™
## <!-- TODO: Find better way, currently for avoide below becomes subtitle -->

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1EbqeoWL5kPaDA8INLWl8g34v3vn83AQ5?usp=sharing)

Install the Diffusers library:
```python
!pip install transformers diffusers accelerate -U
```

Generate an image from text:
```python {all|4-7|8|10|all}{lines:true}
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
  "stabilityai/stable-diffusion-2",
  dtype=torch.float16,
).to(device=torch.device("cuda"))
prompt = "painting depicting the sea, sunrise, ship, artstation, 4k, concept art"

image = pipe(prompt, width=512, height=512).images[0]
display(image)
```

---
level: 2
layout: center
---

# Diffusersã¯æ©Ÿèƒ½ãŒè±Šå¯Œã§æŸ”è»Ÿæ€§ã‚‚é«˜ã„ãŒã€<br />
# ãã®åˆ†ã‚³ãƒ¼ãƒ‰ã®ç†è§£ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã€‚

---
level: 2
---

# [<mdi-github-circle />diffusers/.../pipeline_stable_diffusion.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll mt-10" style="width:100%; height:85%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fhuggingface%2Fdiffusers%2Fblob%2Fmain%2Fsrc%2Fdiffusers%2Fpipelines%2Fstable_diffusion%2Fpipeline_stable_diffusion.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

# [<mdi-github-circle />parediffusers/.../pipeline.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/pipeline.py)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll mt-10" style="width:100%; height:85%;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fpipeline.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
layout: image-right
image: /exps/p-sd2-sample-43.webp
---

# [<mdi-github-circle />PareDiffusers](https://github.com/masaishi/parediffusers)
## <!-- TODO: Find better way, currently for avoide below becomes subtitle -->

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1I-qU3hfF19T42ksIh5FC0ReyKZ2hsJvx?usp=sharing)

Install the PareDiffusers library:
```python
!pip install parediffusers
```

Generate an image from text:
```python {all}{lines:true}
import torch
from parediffusers import PareDiffusionPipeline

pipe = PareDiffusionPipeline.from_pretrained(
  "stabilityai/stable-diffusion-2",
  device=torch.device("cuda"),
  dtype=torch.float16,
)
prompt = "painting depicting the sea, sunrise, ship, artstation, 4k, concept art"

image = pipe(prompt, width=512, height=512)
display(image)
```

---
level: 2
layout: center
---

# ã©ã®ã‚ˆã†ã«ç”»åƒç”ŸæˆãŒè¡Œã‚ã‚Œã¦ã„ã‚‹ã®ã‹ï¼Ÿ

---
level: 2
layout: image-right
image: /exps/p-sd2-sample-43.webp
---

# [<mdi-github-circle />PareDiffusers](https://github.com/masaishi/parediffusers)
## <!-- TODO: Find better way, currently for avoide below becomes subtitle -->

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1I-qU3hfF19T42ksIh5FC0ReyKZ2hsJvx?usp=sharing)

Install the PareDiffusers library:
```python
!pip install parediffusers
```

Generate an image from text:
```python {11}{lines:true}
import torch
from parediffusers import PareDiffusionPipeline

pipe = PareDiffusionPipeline.from_pretrained(
  "stabilityai/stable-diffusion-2",
  device=torch.device("cuda"),
  dtype=torch.float16,
)
prompt = "painting depicting the sea, sunrise, ship, artstation, 4k, concept art"

image = pipe(prompt, width=512, height=512)
display(image)
```

---
level: 2
layout: center
transition: fade
---

<div v-click=1 v-click.hide=2>

[<mdi-github-circle />pipeline.py#L117-L135](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L117-L135)

</div>

````md magic-move {style:'--slidev-code-font-size: 1.2rem; --slidev-code-line-height: 1.5;'}
```python {all}
image = pipe(prompt, width=512, height=512)
```
```python {all}
def __call__(self, prompt: str, height: int = 512, width: int = 512, ...):
	prompt_embeds = self.encode_prompt(prompt)
	latents = self.get_latent(width, height).unsqueeze(dim=0)
	latents = self.denoise(latents, prompt_embeds, ...)
	image = self.vae_decode(latents)
	return image
```
```md
1. `encode_prompt` : ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’CLIPãƒ¢ãƒ‡ãƒ«ã§ã€embeddingã«å¤‰æ›ã™ã‚‹ã€‚
2. `get_latent` : ç”Ÿæˆã—ãŸã„ç”»åƒã‚µã‚¤ã‚ºã®ã€1/8ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚
3. `denoise` : ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®embeddingã‹ã‚‰ã€UNetã¨Schedulerã‚’ç”¨ã„åå¾©çš„ã«ãƒ‡ãƒã‚¤ã‚ºã™ã‚‹ã€‚
4. `vae_decode` : ãƒ‡ãƒã‚¤ã‚ºã•ã‚ŒãŸæ½œåœ¨ç©ºé–“ã‚’ç”»åƒç©ºé–“ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚
```
```md {all}
1. `encode_prompt` : Promptã‚’Embeddingã«å¤‰æ›ã™ã‚‹
2. `get_latent` : ãƒ©ãƒ³ãƒ€ãƒ ãªLatentã‚’ä½œã‚‹
3. `denoise` : Schedulerã¨UNetã‚’ä½¿ã£ã¦ã€ãƒ‡ãƒã‚¤ã‚ºã‚’è¡Œã†
4. `vae_decode` : VAEã§ã€ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹
```
````

---
level: 2
layout: center
---

```md {all|1|2|3|4|all}{lines:false, style:'--slidev-code-font-size: 1.2rem; --slidev-code-line-height: 1.5;'}
1. `encode_prompt` : Promptã‚’Embeddingã«å¤‰æ›ã™ã‚‹
2. `get_latent` : ãƒ©ãƒ³ãƒ€ãƒ ãªLatentã‚’ä½œã‚‹
3. `denoise` : Schedulerã¨UNetã‚’ä½¿ã£ã¦ã€ãƒ‡ãƒã‚¤ã‚ºã‚’è¡Œã†
4. `vae_decode` : VAEã§ã€ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹
```

<img src="/images/ldm-4step-figure.webp" class="mt-5" />

---
level: 2
layout: center
---

# ã¡ã‚‡ã£ã¨ã ã‘ç†è«–

---
level: 2
layout: center
---

Latent Diffusion Model (LDM)ã¨ã¯?

# Latent Space (æ»åœ¨ç©ºé–“)ã§ã€
# <span v-mark.yellow="1">DDPM</span>ã‚’å‹•ã‹ã™ãƒ¢ãƒ‡ãƒ«

---
level: 2
---

Denoising Diffusion Probabilistic Model (DDPM)ã¨ã¯?

<h1 class="!text-7.9">ç”»åƒã«<span v-mark.red="1">ãƒã‚¤ã‚ºã‚’åŠ ãˆ</span>ã€ãã“ã‹ã‚‰<span v-mark.blue="2">å…ƒã®ç”»åƒã«å¾©å…ƒ</span>ã™ã‚‹ãƒ¢ãƒ‡ãƒ«</h1>

<p>éŸ³å£°ãªã©ã®ãƒ‡ãƒ¼ã‚¿å…¨èˆ¬ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ãŒã€ã“ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã¯ç”»åƒã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚</p>

<ul>
	<li><span v-mark.red="1">Diffusion process(æ‹¡æ•£éç¨‹)</span>ã‚’ç”¨ã„ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’è¡Œã†ã€‚ç¢ºç‡éç¨‹ï¼ˆãƒãƒ«ã‚³ãƒ•é€£é–)</li>
	<li><span v-mark.blue="2">Reverse process(é€†æ‹¡æ•£éç¨‹)</span>ã‚’ç”¨ã„ã€ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒã™ã‚‹ã€‚</li>
</ul>

<br />

<img src="/images/ddpm-figure.webp" class="abs-b mb-10 ml-auto mr-auto w-5/6" />

<!-- Reference -->
<p class="text-black text-xs abs-bl w-full mb-6 text-center">
Jonathan Ho, Ajay Jain, Pieter Abbeel: â€œDenoising Diffusion Probabilistic Modelsâ€, 2020; <a href='http://arxiv.org/abs/2006.11239'>arXiv:2006.11239</a>.
</p>

---
level: 2
layout: center
---

# Diffusionã¯å‰å‡¦ç†ã§ã‚‚NNã§ã‚‚ãªã„ã®ã«ã€
# Diffusion Modelã¨å‘¼ã°ã‚Œã‚‹ãŒé¢ç™½ã„

---
level: 2
layout: center
---

Latent Diffusion Model (LDM)ã¨ã¯?

# <span v-mark.green="1">Latent Space (æ»åœ¨ç©ºé–“)</span>ã§ã€
# DDPMã‚’å‹•ã‹ã™ãƒ¢ãƒ‡ãƒ«

---
level: 2
layout: center
transition: fade
---

ç›®çš„é–¢æ•°é–“é•ã„æ¢ã—

<h2>
$$
L_{DM} := \mathbb{E}_{x, \epsilon \sim \mathcal{N}(0, 1),  t}\Big[ \Vert \epsilon - \epsilon_\theta(x_{t},t) \Vert_{2}^{2}\Big] \, .
$$
</h2>

<h2>
$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0, 1),  t}\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t) \Vert_{2}^{2}\Big] \, .
$$
</h2>

<!--
ä¸ŠãŒDiffusion Model
ä¸‹ãŒLatent Diffusion Model

\mathcal  = ç­†è¨˜ä½“(ã‚«ãƒªã‚°ãƒ©ãƒ•ã‚£ãƒ¼)ã€ãªã‚“ã¦èª­ã‚ã°ã„ã„? AIã«ãŠã„ã¦ã¯Encorderã¨èª­ã‚“ã˜ã‚ƒã£ã¦ã„ã„?
\mathbb{E}_{x ã§ã¯ãªãã€VAE Encorderã‚’é€šã—ãŸlatent spaceã‚’æœŸå¾…å€¤ã®è¨ˆç®—ã«ä½¿ã£ã¦ã„ã‚‹ã€‚
-->

---
level: 2
layout: center
transition: fade
---

Latent Diffusion Model (LDM)

<h2>
$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0, 1),  t}\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t) \Vert_{2}^{2}\Big] \, .
$$
</h2>

<!--
\mathcal{E} = VAE Encorder
\mathcal{N} = ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚º
\epsilon_\theta = VAE Decoder

E(æœŸå¾…å€¤): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ ãŒEncorderã‚’é€šã—ã¦latent spaceã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ã€‚
VAE Decoderã‚’é€šã™ã¨ãã€t=timestepã‚’è€ƒæ…®ã—ã¦ã„ã‚‹ã€‚
-->

---
level: 2
layout: center
transition: fade
---

Latent Diffusion Model (LDM)

<h2>
$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0, 1),  t}\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t) \Vert_{2}^{2}\Big] \, .
$$
</h2>

<!--
\mathcal{E} = VAE Encorder
\mathcal{N} = ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚º
\epsilon_\theta = VAE Decoder

E(æœŸå¾…å€¤): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ ãŒEncorderã‚’é€šã—ã¦latent spaceã«å¤‰æ›ã•ã‚Œã¦ã„ã‚‹ã€‚
VAE Decoderã‚’é€šã™ã¨ãã€t=timestepã‚’è€ƒæ…®ã—ã¦ã„ã‚‹ã€‚
-->

---
level: 2
layout: center
---

Latent Diffusion Model (LDM) with Conditioning

<h2>
$$
L_{LDM} := \mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0, 1), t }\Big[ \Vert \epsilon - \epsilon_\theta(z_{t},t, \tau_\theta(y)) \Vert_{2}^{2}\Big] \, ,
$$
</h2>

<v-clicks every="1" at="1">

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) \cdot V
$$

$$
\begin{equation*}
Q = W^{(i)}_Q \cdot  \varphi_i(z_t), \; K = W^{(i)}_K \cdot \tau_\theta(y),
  \; V = W^{(i)}_V \cdot \tau_\theta(y) . \nonumber
%
\end{equation*}
$$

</v-clicks>

<!--
Conditioningã€ã¤ã¾ã‚Špromptã‚„semantic mapã€repres entations, imagesãªã©ã‚’è€ƒæ…®ã—ã¦ã„ã‚‹ã€‚
-->

---
level: 2
transition: fade
---

<div class="flex flex-col !justify-between w-full h-120">
	<div>
		<img src="/images/ddpm-figure.webp" class="ml-auto mr-auto h-26" />
		<!-- Reference -->
		<p class="text-black text-xs w-full mt-6 text-center">
		Jonathan Ho, Ajay Jain, Pieter Abbeel: â€œDenoising Diffusion Probabilistic Modelsâ€, 2020; <a href='http://arxiv.org/abs/2006.11239'>arXiv:2006.11239</a>.
		</p>
	</div>
	<div v-click>
		<span class="text-xs ml-27.5% mt-0 mb-0">Transformerã®æ¬¡ã«æ­»ã¬ã»ã©ç›®ã«ã—ãŸStable Diffusionã®å›³</span>
		<img src="/images/stable-diffusion-figure.webp" alt="Stable Diffusion Figure" class="ml-auto mr-auto h-48 object-contain" />
		<p class="text-black text-xs w-full mt-6 text-center">
		Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer: â€œHigh-Resolution Image Synthesis with Latent Diffusion Modelsâ€, 2021; <a href='http://arxiv.org/abs/2112.10752'>arXiv:2112.10752</a>.
		</p>
	</div>
</div>

---
level: 2
layout: center
---

<iframe frameborder="0" scrolling="no" style="width:100%; height:163px;" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2F035772c684ae8d16c7c908f185f6413b72658126%2Fsrc%2Fparediffusers%2Fpipeline.py%23L131-L134&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<div class="w-full flex flex-col justify-center mt-10.7">
<img src="/images/stable-diffusion-figure.webp" alt="Stable Diffusion Figure" class="h-48 object-contain" />
<p class="text-black text-xs w-full mt-6 text-center">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer: â€œHigh-Resolution Image Synthesis with Latent Diffusion Modelsâ€, 2021; <a href='http://arxiv.org/abs/2112.10752'>arXiv:2112.10752</a>.
</p>
</div>

---
level: 2
layout: center
---

Latent Space(æ»åœ¨ç©ºé–“)ã¨ã¯?

# å…¥åŠ›ç”»åƒã®ç‰¹å¾´ã‚’æŠ½å‡ºã—ãŸç©ºé–“

<!--
TODO: VAEã‚’é€šã—ãŸç”»åƒã®å¹³å‡ã‚’ã¨ã£ãŸç”»åƒã‚’ç”¨æ„ã™ã‚‹ã€‚
-->

---
level: 2
layout: center
transition: fade
---

4ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ã‹ã‚‹ç”»åƒç”Ÿæˆã®æµã‚Œ

<h1 class="!text-7">
ã‚¹ãƒ†ãƒƒãƒ—1: Promptã‚’Embeddingã«å¤‰æ›ã™ã‚‹<br />
ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ©ãƒ³ãƒ€ãƒ ãªLatentã‚’ä½œã‚‹<br />
ã‚¹ãƒ†ãƒƒãƒ—3: Schedulerã¨UNetã‚’ä½¿ã£ã¦ã€ãƒ‡ãƒã‚¤ã‚ºã‚’è¡Œã†<br />
ã‚¹ãƒ†ãƒƒãƒ—4: VAEã§ã€ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹
</h1>

---
level: 2
layout: center
---

ã“ã‚Œã‹ã‚‰ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®æµã‚Œ

<h1 class="!text-7">
ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt<br />
ã‚¹ãƒ†ãƒƒãƒ—2: get_latent<br />
ã‚¹ãƒ†ãƒƒãƒ—3: denoise<br />
ã‚¹ãƒ†ãƒƒãƒ—4: vae_decode
</h1>

---
layout: cover
title: "ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt"
background: /backgrounds/pipeline.webp
---

<h1>ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt</h1>

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Pipeline, cyberpunk theme, best quality, high resolution, concept art</p>

---
level: 2
layout: center
transition: fade
---

ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’embeddingã«å¤‰æ›ã™ã‚‹

---
level: 2
layout: center
---

ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ãŒæ‰±ã„ã‚„ã™ã„å½¢ã«å¤‰æ›ã™ã‚‹

---
level: 2
layout: center
---

å¿…è¦ãªã‚‚ã®
# - [CLIPTokenizer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_clip.py#L251)
# - [CLIPTextModel](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py)
##
From [<mdi-github-circle />huggingface/transformers](https://github.com/huggingface/transformers/tree/main)

---
level: 2
layout: two-cols
transition: fade
---

<h1 class="!text-8.3">ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt</h1>
<p>encode_promptå†…ã§ã€åˆ¥ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ã„ã‚‹</p>

::right::

[<mdi-github-circle />pipeline.py#L41-L48](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L48)

```python {all|45|45-46}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
```

---
level: 2
layout: two-cols
transition: fade
---

<h1 class="!text-8.3">ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt</h1>
<p>å¿…è¦ãªã‚‚ã®ã¯ã©ã“ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹?</p>

::right::

[<mdi-github-circle />pipeline.py#L41-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L57)

```python {all|54|54,56}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
 
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
layout: two-cols
transition: fade
---

<h1 class="!text-8.3">ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt</h1>
<p>å¿…è¦ãªã‚‚ã®ã¯ã©ã“ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹?</p>

<v-clicks every="1" at="1">

- L54: `CLIPTokenizer`: ãƒ†ã‚­ã‚¹ãƒˆ(prompt)ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€‚ãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ã“ã¨ã§ã€AIã«æ‰±ã„ã‚„ã™ãã•ã›ã‚‹ã€‚

- L56: `CLIPTextModel`: è¨€èªã¨ç”»åƒã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚ç”»åƒç”Ÿæˆã«ãŠã„ã¦ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä½œã‚ŠãŸã„ç”»åƒã®è¡¨ç¾ï¼ˆembeddingï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L21-L39](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L21-L39)

```python {4,5|4|4,5}{lines:false,at:1}
@classmethod
def from_pretrained(cls, model_name, device=torch.device("cuda"), dtype=torch.float16):
	# Ommit comments
	tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")
	text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder")
	scheduler = PareDDIMScheduler.from_config(model_name, subfolder="scheduler")
	unet = PareUNet2DConditionModel.from_pretrained(model_name, subfolder="unet")
	vae = PareAutoencoderKL.from_pretrained(model_name, subfolder="vae")
	return cls(tokenizer, text_encoder, scheduler, unet, vae, device, dtype)
```

[<mdi-github-circle />pipeline.py#L50-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L50-L57)

```python {54,56|54|54,56}{lines:true,startLine:50,at:1}
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
layout: two-cols
---

<h1 class="!text-8.3">ã‚¹ãƒ†ãƒƒãƒ—1: encode_prompt</h1>
<p>ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿å…¨ä½“ã®æµã‚Œã‚’ç†è§£ã™ã‚‹</p>

- L54: `CLIPTokenizer`: ãƒ†ã‚­ã‚¹ãƒˆ(prompt)ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€‚ãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ã“ã¨ã§ã€AIã«æ‰±ã„ã‚„ã™ãã•ã›ã‚‹ã€‚

- L56: `CLIPTextModel`: è¨€èªã¨ç”»åƒã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚ç”»åƒç”Ÿæˆã«ãŠã„ã¦ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ä½œã‚ŠãŸã„ç”»åƒã®è¡¨ç¾ï¼ˆembeddingï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚


<v-clicks every="1" at="2">

- L46: ã‚·ãƒ³ãƒ—ãƒ«ã«ã™ã‚‹ãŸã‚ã«ã€negative_promptã¯ç©ºã®æ–‡å­—åˆ—ã¨ã—ã¦ã„ã¾ã™ã€‚

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L34-L35](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L34-L35)

```python {all}{lines:true,startLine:34,at:1}
	tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")
	text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder")
```

[<mdi-github-circle />pipeline.py#L41-L57](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L41-L57)

```python {|all|46|all}{lines:true,startLine:41,at:1}
def encode_prompt(self, prompt: str):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	prompt_embeds = self.get_embes(prompt, self.tokenizer.model_max_length)
	negative_prompt_embeds = self.get_embes([''], prompt_embeds.shape[1])
	prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
	return prompt_embeds
 
def get_embes(self, prompt, max_length):
	"""
	Encode the text prompt into embeddings using the text encoder.
	"""
	text_inputs = self.tokenizer(prompt, padding="max_length", max_length=max_length, truncation=True, return_tensors="pt")
	text_input_ids = text_inputs.input_ids.to(self.device)
	prompt_embeds = self.text_encoder(text_input_ids)[0].to(dtype=self.dtype, device=self.device)
	return prompt_embeds
```

---
level: 2
---

<iframe frameborder="0" scrolling="no" class="emg-iframe-text-inputs" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Funderstand-stable-diffusion-slidev-notebooks%2Fblob%2Fmain%2Fembed%2Fch5-text_inputs.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>


<style>
	.emg-iframe-text-inputs {
		transform: scale(0.9) translate(-50%, -50%); /* Apply both transformations */
		transform-origin: top left;
		position: absolute;
		top: 50%;
		left: 50%;
		width: 100%;
		height: 100%;
	}
</style>

---
level: 2
---

<iframe frameborder="0" scrolling="no" class="emg-iframe-prompt-embeds" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Funderstand-stable-diffusion-slidev-notebooks%2Fblob%2Fmain%2Fembed%2Fch5-prompt_embeds.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<style>
	.emg-iframe-prompt-embeds {
		transform: scale(0.8) translate(-50%, -50%);
		transform-origin: top left;
		position: absolute;
		top: 57%;
		left: 50%;
		width: 100%;
		height: 130%;
	}
</style>

---
level: 2
layout: center
---

<iframe frameborder="0" scrolling="yes" class="overflow-scroll emg-iframe-play-prompt-embeds" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fnotebooks%2Fch0.0.2_Play_prompt_embeds.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<style>
	.emg-iframe-play-prompt-embeds {
		transform: scale(0.5) translate(-50%, -50%); /* Apply both transformations */
		transform-origin: top left;
		position: absolute;
		top: 50%;
		left: 50%;
		width: 100%;
		height: 160%;
	}
</style>

<!--
ã¾ã‚‹ã§Word2Vec
-->

---
layout: cover
title: "ã‚¹ãƒ†ãƒƒãƒ—2: get_latent"
background: /backgrounds/scheduler.webp
---

# ã‚¹ãƒ†ãƒƒãƒ—2: get_latent

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Scheduler, flat vector illustration, best quality, high resolution</p>

---
level: 2
layout: center
---

ã‚¹ãƒ†ãƒƒãƒ—2: get_latent
# 1/8ã®ã‚µã‚¤ã‚ºã®ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆ

---
level: 2
layout: center
---

å¿…è¦ãªã‚‚ã®
# torch.randn

---
level: 2
layout: custom-two-cols
leftPercent: 0.4
---

<h1 class="!text-8.3">ã‚¹ãƒ†ãƒƒãƒ—2: get_latent</h1>
<p>ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿å…¨ä½“ã®æµã‚Œã‚’ç†è§£ã™ã‚‹</p>

<v-clicks every="1">

- L63: 1/8ã®ã‚µã‚¤ã‚ºã®ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’ç”Ÿæˆ

<img src="/exps/latent.webp" class="mt-5 h-48 object-contain" />

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L59-L65](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L59-L65)


```python {all|63|all}{lines:true,startLine:59,at:1,style:'--slidev-code-font-size: 1rem; --slidev-code-line-height: 1.5;'}
def get_latent(self, width: int, height: int):
	"""
	Generate a random initial latent tensor to start the diffusion process.
	"""
	return torch.randn((4, width // 8, height // 8)).to(
		device=self.device, dtype=self.dtype
	)
```

---
layout: cover
title: "ã‚¹ãƒ†ãƒƒãƒ—3: denoise"
background: /backgrounds/unet.webp
---

# ã‚¹ãƒ†ãƒƒãƒ—3: denoise

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: UNet, watercolor painting, detailed, brush strokes, best quality, high resolution</p>

---
level: 2
layout: center
---

ã‚¹ãƒ†ãƒƒãƒ—3: denoise
# Schedulerã¨UNetã‚’ä½¿ã£ã¦ã€ãƒ‡ãƒã‚¤ã‚ºã‚’è¡Œã†

---
level: 2
layout: center
---

<img src="/exps/denoised_latents_with_index.webp" class="h-96 object-contain mr-auto ml-auto" />

[<mdi-github-circle />understand-stable-diffusion-slidev-notebooks/denoise.ipynb](https://github.com/masaishi/understand-stable-diffusion-slidev-notebooks/blob/main/denoise.ipynb)

---
level: 2
layout: center
---

<img src="/exps/decoded_images_with_index.webp" class="h-100 object-contain mr-auto ml-auto" />

[<mdi-github-circle />understand-stable-diffusion-slidev-notebooks/denoise.ipynb](https://github.com/masaishi/understand-stable-diffusion-slidev-notebooks/blob/main/denoise.ipynb)

---
level: 2
layout: center
---

å¿…è¦ãªã‚‚ã®
# [<mdi-github-circle />scheduler.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/scheduler.py)
# [<mdi-github-circle />unet.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/unet.py)

---
level: 2
layout: center
---

ã‚¹ãƒ†ãƒƒãƒ—3: denoise
# ãã®2ã¤ã®è©³ç´°ã¯ç½®ã„ã¦ãŠãã€å…¨ä½“ã®æµã‚Œ

---
level: 2
layout: custom-two-cols
leftPercent: 0.5
transition: fade
---

# ã‚¹ãƒ†ãƒƒãƒ—3: denoise
å¿…è¦ãªã‚‚ã®ã¯ã©ã“ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹?

<v-clicks every="1">

- L86: UNet

- L91: Scheduler

</v-clicks>

::right::


[<mdi-github-circle />pipeline.py#L75-L93](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L75-L93)

```python {all|86|86,91}{lines:true,startLine:75,at:1}
@torch.no_grad()
def denoise(self, latents, prompt_embeds, num_inference_steps=50, guidance_scale=7.5):
	"""
	Iteratively denoise the latent space using the diffusion model to produce an image.
	"""
	timesteps, num_inference_steps = self.retrieve_timesteps(num_inference_steps)

	for t in timesteps:
		latent_model_input = torch.cat([latents] * 2)
		
		# Predict the noise residual for the current timestep
		noise_residual = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)
		uncond_residual, text_cond_residual = noise_residual.chunk(2)
		guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

		# Update latents by reversing the diffusion process for the current timestep
		latents = self.scheduler.step(guided_noise_residual, t, latents)[0]

	return latents
```

---
level: 2
layout: custom-two-cols
leftPercent: 0.5
transition: fade
---

# ã‚¹ãƒ†ãƒƒãƒ—3: denoise
å¿…è¦ãªã‚‚ã®ã¯ã©ã“ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹?

- L86: UNet2DConditionModel

- L91: DDIMScheduler


::right::

[<mdi-github-circle />pipeline.py#L21-L39](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py#L21-L39)

```python {6,7}{lines:false,at:1}
@classmethod
def from_pretrained(cls, model_name, device=torch.device("cuda"), dtype=torch.float16):
	# Ommit comments
	tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder="tokenizer")
	text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder="text_encoder")
	scheduler = PareDDIMScheduler.from_config(model_name, subfolder="scheduler")
	unet = PareUNet2DConditionModel.from_pretrained(model_name, subfolder="unet")
	vae = PareAutoencoderKL.from_pretrained(model_name, subfolder="vae")
	return cls(tokenizer, text_encoder, scheduler, unet, vae, device, dtype)
```

[<mdi-github-circle />pipeline.py#L82-L93](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L82-L93)

```python {86,91}{lines:true,startLine:82,at:1}
	for t in timesteps:
		latent_model_input = torch.cat([latents] * 2)
		
		# Predict the noise residual for the current timestep
		noise_residual = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)
		uncond_residual, text_cond_residual = noise_residual.chunk(2)
		guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

		# Update latents by reversing the diffusion process for the current timestep
		latents = self.scheduler.step(guided_noise_residual, t, latents)[0]

	return latents
```

---
level: 2
layout: custom-two-cols
leftPercent: 0.5
---

# ã‚¹ãƒ†ãƒƒãƒ—3: denoise
ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿å…¨ä½“ã®æµã‚Œã‚’ç†è§£ã™ã‚‹

<v-clicks every="1">

- L80: Schedulerã‚’ä½¿ã„timestepsã®å–å¾— <br />(<span class="text-sm">Schedulerã«ã¤ã„ã¦ã¯å¾Œè¿°</span>)

- L82: timestepsã®é•·ã•åˆ†ãƒ«ãƒ¼ãƒ—<br />(<span class="text-sm">timestepsã®é•·ã•åˆ† = num_inference_steps</span>)

- L86: UNetã§ãƒ‡ãƒã‚¤ã‚º <br />(<span class="text-sm">UNetã«ã¤ã„ã¦ã¯å¾Œè¿°</span>)

- L88: ã©ã‚Œã ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è€ƒæ…®ã™ã‚‹ã‹ã‚’è¨ˆç®— <br />(<span class="text-3">å‚è€ƒ: 
Jonathan Ho, Tim Salimans: â€œClassifier-Free Diffusion Guidanceâ€, 2022; <a href='http://arxiv.org/abs/2207.12598'>arXiv:2207.12598</a>.</span>)

- L91: Schedulerã«ã‚ˆã£ã¦ã€ãƒ‡ãƒã‚¤ã‚ºã®å¼·ã•ã‚’æ±ºå®š

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L82-L93](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L82-L93)

```python {all|80|82|86|88|91|all}{lines:true,startLine:75,at:1}
@torch.no_grad()
def denoise(self, latents, prompt_embeds, num_inference_steps=50, guidance_scale=7.5):
	"""
	Iteratively denoise the latent space using the diffusion model to produce an image.
	"""
	timesteps, num_inference_steps = self.retrieve_timesteps(num_inference_steps)

	for t in timesteps:
		latent_model_input = torch.cat([latents] * 2)
		
		# Predict the noise residual for the current timestep
		noise_residual = self.unet(latent_model_input, t, encoder_hidden_states=prompt_embeds)
		uncond_residual, text_cond_residual = noise_residual.chunk(2)
		guided_noise_residual = uncond_residual + guidance_scale * (text_cond_residual - uncond_residual)

		# Update latents by reversing the diffusion process for the current timestep
		latents = self.scheduler.step(guided_noise_residual, t, latents)[0]

	return latents
```

---
level: 2
---

# <span class="text-3xl">[<mdi-github-circle />scheduler.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/scheduler.py)</span>
ãƒ‡ãƒã‚¤ã‚ºã®å¼·ã•ã‚’æ±ºå®š

<iframe frameborder="0" scrolling="yes" class="overflow-scroll iframe-full-code" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fscheduler.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

[<mdi-github-circle />scheduler.py#L40-L59](https://github.com/masaishi/parediffusers/blob/17e8ece5e6104fbec34d64c4d87545f340b0ea50/src/parediffusers/scheduler.py#L40-L59)

```python {all|49|49,50}{lines:true, startLine:40}
def step(
	self,
	model_output: torch.FloatTensor,
	timestep: int,
	sample: torch.FloatTensor,
) -> list:
	"""Perform a single step of denoising in the diffusion process."""
	prev_timestep = timestep - self.config.num_train_timesteps // self.num_inference_steps

	alpha_prod_t = self.alphas_cumprod[timestep]
	alpha_prod_t_prev = self.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.final_alpha_cumprod

	beta_prod_t = 1 - alpha_prod_t
	pred_original_sample = (alpha_prod_t**0.5) * sample - (beta_prod_t**0.5) * model_output
	pred_epsilon = (alpha_prod_t**0.5) * model_output + (beta_prod_t**0.5) * sample

	pred_sample_direction = (1 - alpha_prod_t_prev) ** (0.5) * pred_epsilon
	prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction

	return prev_sample, pred_original_sample
```

---
level: 2
layout: center
transition: fade
---

<div class="flex content-around gap-6">

<img src="/exps/alpha_prod_t.webp" class="h-64 object-contain ml-auto mr-auto" />

<img src="/exps/alpha_prod_t_prev.webp" class="h-64 object-contain ml-auto mr-auto" />

</div>

<p class="text-center">
<a src="https://github.com/masaishi/understand-stable-diffusion-slidev-notebooks/blob/main/scheduler.ipynb"><mdi-github-circle />understand-stable-diffusion-slidev-notebooks/scheduler.ipynb</a>
</p>

---
level: 2
layout: center
transition: fade
---

<div class="flex content-around gap-6">
<h1 class="!text-16 !mt-auto !mb-auto">âˆ’</h1>
<img src="/exps/alpha_prod_t.webp" class="h-64 object-contain ml-auto mr-auto" />
<h1 class="!text-16 !mt-auto !mb-auto">+</h1>
<img src="/exps/alpha_prod_t_prev.webp" class="h-64 object-contain ml-auto mr-auto" />
</div>

<p class="text-center">
<a src="https://github.com/masaishi/understand-stable-diffusion-slidev-notebooks/blob/main/scheduler.ipynb"><mdi-github-circle />understand-stable-diffusion-slidev-notebooks/scheduler.ipynb</a>
</p>

---
level: 2
layout: center
---

<div class="flex content-around gap-6">
<img src="/exps/alpha_diff.webp" class="h-64 object-contain ml-auto mr-auto" />

</div>

<p class="text-center">
<a src="https://github.com/masaishi/understand-stable-diffusion-slidev-notebooks/blob/main/scheduler.ipynb"><mdi-github-circle />understand-stable-diffusion-slidev-notebooks/scheduler.ipynb</a>
</p>

---
level: 2
---

<iframe frameborder="0" scrolling="no" class="scale-40 -translate-y-1/2 absolute top-54% right-25% w-full h-240%" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Funderstand-stable-diffusion-slidev-notebooks%2Fblob%2F606a033780f0c9aa0681fd1468f91f3961a73a3f%2Fembed%2Fwith_scheduler.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>


<iframe frameborder="0" scrolling="no" class="scale-40 -translate-y-1/2 absolute top-50% left-25% w-full h-240%" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Funderstand-stable-diffusion-slidev-notebooks%2Fblob%2Fmain%2Fembed%2Fwithout_scheduler.ipynb&style=github&type=ipynb&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<!--
Schedulerã‚’ç”¨ã„ã¦å­¦ç¿’ã‚’è¡Œãªã£ã¦ã„ã‚‹ã®ã«ã€ãã‚Œã‚’ãªã—ã§æ¨è«–ã—æ¯”è¼ƒã—ã¦ã„ã‚‹ã®ã¯ãšã‚‹ã„æ°—ãŒã—ã¾ã™ãŒã€‚
ãã‚Œã§ã‚‚è¼ªéƒ­ãªã©ã¯ã©ã†ã‚ˆã†ãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã¯ã§ãã¾ã™ã€‚ã—ã‹ã—ã€æ˜ã‚‰ã‹ã«SchedulerãŒãªã„ã¨ãã‚Œã„ãªç”»åƒã®ç”ŸæˆãŒå‡ºæ¥ã¦ã„ãªã„ã®ãŒã‚ã‹ã‚‹ã‹ã¨æ€ã„ã¾ã™ã€‚
-->

---
level: 2
layout: center
---

<h1 class="mb-0">ãªãœ<code>ratio = 1.5</code>å‰å¾ŒãŒã„ã„ã‹ã¯åˆ†ã‹ã‚‰ãªã„</h1>

<img class="h-100 object-contain -mb-5 ml-auto mr-auto" src="/exps/custom_denoise_different_ratio.webp" />

<p class="text-center">
	<a src="https://github.com/masaishi/understand-stable-diffusion-slidev-notebooks/blob/main/scheduler_necessity.ipynb">
		<mdi-github-circle />understand-stable-diffusion-slidev-notebooks/scheduler_necessity.ipynb
	</a>
</p>

---
level: 2
---

# [<mdi-github-circle />unet.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/unet.py)
ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã«ä½¿ã‚ã‚Œã‚‹

<iframe frameborder="0" scrolling="yes" class="overflow-scroll iframe-full-code" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Funet.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
layout: image
image: /images/unet-figure.webp
backgroundSize: 70%
class: 'text-black'
---

<!-- Reference -->
<p class="text-black text-xs abs-bl w-full mb-6 text-center">
Olaf Ronneberger, Philipp Fischer, Thomas Brox: â€œU-Net: Convolutional Networks for Biomedical Image Segmentationâ€, 2015; <a href='http://arxiv.org/abs/1505.04597'>arXiv:1505.04597</a>.
</p>

---
level: 2
layout: center
---

# æœ¬å½“ã«Uã«ãªã£ã¦ã‚‹?

```python
init             torch.Size([2, 4, 64, 64])
conv_in          torch.Size([2, 320, 64, 64])

down_blocks_0    torch.Size([2, 320, 32, 32])
down_blocks_1    torch.Size([2, 640, 16, 16])
down_blocks_2    torch.Size([2, 1280, 8, 8])
down_blocks_3    torch.Size([2, 1280, 8, 8])

mid_block        torch.Size([2, 1280, 8, 8])

up_blocks0       torch.Size([2, 1280, 16, 16])
up_blocks1       torch.Size([2, 1280, 32, 32])
up_blocks2       torch.Size([2, 640, 64, 64])
up_blocks3       torch.Size([2, 320, 64, 64])

conv_out         torch.Size([2, 4, 64, 64])
```

[<mdi-github-circle />understand-stable-diffusion-slidev-notebooks/unet.ipynb](https://github.com/masaishi/understand-stable-diffusion-slidev-notebooks/blob/main/unet.ipynb)

---
level: 2
---

# Resnetã¨Transformerã‚’ä½¿ã„UNetã‚’ä½œæˆ

<iframe frameborder="0" scrolling="yes" class="emg-res-transformer" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2F675b3fdaf4435e9982f92ff933f78db64f16a980%2Fsrc%2Fparediffusers%2Fmodels%2Funet_2d_blocks.py%23L114-L141&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

<style>
	.emg-res-transformer {
		transform: scale(0.68) translate(-50%, -50%);
		transform-origin: top left;
		position: absolute;
		top: 63%;
		left: 50%;
		width: 100%;
		height: 130%;
	}
</style>

---
layout: cover
title: "ã‚¹ãƒ†ãƒƒãƒ—4: vae_decode"
background: /backgrounds/vae.webp
---

# ã‚¹ãƒ†ãƒƒãƒ—4: vae_decode

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: VAE, abstract style, highly detailed, colors and shapes</p>

---
level: 2
layout: center
---

ã‚¹ãƒ†ãƒƒãƒ—4: vae_decode
# VAEã§ã€ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹

---
level: 2
layout: custom-two-cols
leftPercent: 0.4
---

<h1 class="!text-7.8">ã‚¹ãƒ†ãƒƒãƒ—4: vae_decode</h1>
<p>ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿å…¨ä½“ã®æµã‚Œã‚’ç†è§£ã™ã‚‹</p>

<v-clicks every="1">

- L112: VAEã§ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰

- L113: æ­£è¦åŒ–ã—ã¦å­¦ç¿’ã—ã¦ã„ã‚‹ã®ã§ã€é€†æ­£è¦åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

- L114: ãƒ†ãƒ³ã‚½ãƒ«ã‹ã‚‰PIL.Imageã«å¤‰æ›

</v-clicks>

::right::

[<mdi-github-circle />pipeline.py#L107-L105](https://github.com/masaishi/parediffusers/blob/035772c684ae8d16c7c908f185f6413b72658126/src/parediffusers/pipeline.py#L107-L115)

```python {all|112|113|114}{lines:true,startLine:107,at:1}
@torch.no_grad()
def vae_decode(self, latents):
	"""
	Decode the latent tensors using the VAE to produce an image.
	"""
	image = self.vae.decode(latents / self.vae.config.scaling_factor)[0]
	image = self.denormalize(image)
	image = self.tensor_to_image(image)
	return image
```

---
level: 2
layout: center
---

# Variational Autoencoder
å¤‰åˆ†è‡ªå·±ç¬¦å·åŒ–å™¨ (æ—¥æœ¬èªè¨³ã‹ã£ã“ã„ã„!)

---
level: 2
---

# [<mdi-github-circle />vae.py](https://github.com/masaishi/parediffusers/blob/main/src/parediffusers/vae.py)

<iframe frameborder="0" scrolling="yes" class="overflow-scroll iframe-full-code" allow="clipboard-write" src="https://emgithub.com/iframe.html?target=https%3A%2F%2Fgithub.com%2Fmasaishi%2Fparediffusers%2Fblob%2Fmain%2Fsrc%2Fparediffusers%2Fvae.py&style=github&type=code&showBorder=on&showLineNumbers=on&showFileMeta=on&showFullPath=on&showCopy=on"></iframe>

---
level: 2
---

VAEã‚’å¤‰ãˆã¦ã‚‚ç”»åƒãŒç”Ÿæˆã§ãã‚‹è©±?
ç”»åƒç”Ÿæˆæ¨è«–ã‚µãƒ¼ãƒãƒ¼ã®VAEã‚’å¤‰æ›´æ©Ÿèƒ½è¿½åŠ ã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§æœ€åˆã«ã‚„ã£ãŸã“ã¨ãªã®ã§ã¡ã‚‡ã£ã¨è©±ã›ã‚‹ã‹ã‚‚ã€‚

---
layout: cover
title: ã¾ã¨ã‚
background: /backgrounds/summary.webp
---

# 9. ã¾ã¨ã‚

<p class="text-xs abs-bl w-full mb-6 text-center">Prompt: Summary, long-exposure photography, masterpieces</p>

---
level: 2
layout: center
---

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚³ãƒ¼ãƒ‰ã‚’èª­ã‚€ã®æ¥½ã—ã„!

---
level: 2
layout: center
---

## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è‡³ã‚‹æ‰€ã§ã€è«–æ–‡ãŒå¼•ç”¨

[<mdi-github-circle />diffusers/.../pipeline.py](https://github.com/masaishi/parediffusers/blob/9e32721a4b1a63baf499517384e2a2acd9c08dae/src/parediffusers/pipeline.py)

<img src="/images/diffusers-code-arxiv.webp" class="mt-5 h-92 object-contain" />

---
level: 2
layout: center
---

ã¾ã¨ã‚
# 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
# 2. ãƒ©ãƒ³ãƒ€ãƒ ãªæ½œåœ¨ç©ºé–“ã®ç”Ÿæˆ
# 3. UNetã‚’ç”¨ã„ã¦ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°
# 4. VAEã§ã€ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹

---
level: 2
layout: center
---

# ã”æ¸…è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼
